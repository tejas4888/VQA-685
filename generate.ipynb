{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEo6hjU9h+T0HmRxOc2Nnx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas4888/VQA-685/blob/main/generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqEnWjq7thoW",
        "outputId": "182b9ccb-e588-4074-87c5-4ae65cdc1dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Loading PathVQA\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive \n",
        "\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def load_data(path):\n",
        "\n",
        "    # path = \"/content/drive/MyDrive/PathVQA/split/\"    \n",
        "\n",
        "    train_path = path + 'qas/train/'\n",
        "    test_path = path + 'qas/test/'\n",
        "    val_path = path + 'qas/val/'\n",
        "\n",
        "    print (\"\\nLoading PathVQA\")\n",
        "    train_qa = pd.read_pickle(train_path + \"train_qa.pkl\")\n",
        "    test_qa = pd.read_pickle(test_path + \"test_qa.pkl\")\n",
        "    val_qa = pd.read_pickle(val_path + \"val_qa.pkl\")\n",
        "    ans2label = pd.read_pickle(path+'qas/ans2label.pkl')\n",
        "    \n",
        "    train = []\n",
        "    for row in train_qa:\n",
        "        if row['answer'] in ans2label:\n",
        "            train.append([ row['image'], row['question'], int(ans2label[row['answer']]) ] )\n",
        "\n",
        "    test = []\n",
        "    for row in test_qa:\n",
        "        if row['answer'] in ans2label:\n",
        "            test.append([row['image'], row['question'], int(ans2label[row['answer']]) ])\n",
        "\n",
        "    val = []\n",
        "    for row in val_qa:\n",
        "            if row['answer'] in ans2label:\n",
        "                val.append([row['image'], row['question'], int(ans2label[row['answer']]) ])\n",
        "\n",
        "    # train = np.array()\n",
        "\n",
        "    # return train_qa, test_qa, val_qa, ans2label\n",
        "    return np.asarray(train), np.asarray(test), np.asarray(val), ans2label\n",
        "\n",
        "mount_drive()\n",
        "path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "train, test, val, ans2label = load_data(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Accuracy\n",
        "\n",
        "Predict the most common class"
      ],
      "metadata": {
        "id": "iKKszQW54yBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnts = {}\n",
        "for i in range(0, len(ans2label)):\n",
        "    cnts[i] = 0\n",
        "\n",
        "for example in train:\n",
        "    label = int(example[2])\n",
        "    cnts[label]+=1\n",
        "\n",
        "max = 0\n",
        "max_idx = 0\n",
        "for key in cnts:\n",
        "    if (cnts[key]>max):\n",
        "        max_idx = key\n",
        "        max = cnts[key]\n",
        "\n",
        "print (max_idx, max)\n",
        "\n",
        "test_baseline_acc = 0.0\n",
        "for example in test:\n",
        "    if (int(example[2]) == max_idx):\n",
        "        test_baseline_acc += 1.0\n",
        "test_baseline_acc = test_baseline_acc/len(test)\n",
        "\n",
        "print (f'Baseline test accuracy is {test_baseline_acc}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bve4yz-qtuT0",
        "outputId": "3ba8bfd4-da3c-41c8-efec-8b772c35fe93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 5428\n",
            "Baseline test accuracy is 0.3177119228118539.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Features"
      ],
      "metadata": {
        "id": "2kKmZv0b47bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I4YtRVk6y3v",
        "outputId": "84cb8900-215a-48f0-e14c-7bfd30fbbcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 274 kB 13.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=83cefebb7382b701a4fd0287b9512f61b25d3ea7bf4aff3514cb0abb239296b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-echxfiqu\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-echxfiqu\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.7.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 35.6 MB/s \n",
            "\u001b[?25hCollecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n",
            "Collecting regex>=2020.1.8\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 37.0 MB/s \n",
            "\u001b[?25hCollecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 25.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.4.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (0.29.24)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.6.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.42.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.1.1)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp37-cp37m-linux_x86_64.whl size=5722397 sha256=00bef859690e4769d40d5d64df9f7c76448e661028aff5d1945465fc226bfd49\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n8m3c60s/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=3ab2fe32150c64f36bd4dd1287116219424809a97e51f406e1b71480099724bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=1f0396279a1d2f6708d4b9ee90fcfe74cdc23885dc8cce7e9fe9c8977ad0d60f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20211023 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.3.2 regex-2021.11.10 typed-ast-1.5.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "import torch, torchvision\n",
        "import yaml\n",
        "import json \n",
        "import cv2\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.structures.image_list import ImageList\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "# from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
        "from detectron2.structures.boxes import Boxes\n",
        "from detectron2.layers import nms\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg"
      ],
      "metadata": {
        "id": "WrBpA7hg63iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PretrainedCNN:\n",
        "\n",
        "    def __init__(self, cfg_path):\n",
        "\n",
        "        self.cfg = self.load_config_and_model_weights(cfg_path)\n",
        "        self.model = self.get_model(self.cfg)\n",
        "\n",
        "    def load_config_and_model_weights(self, cfg_path):\n",
        "        cfg = get_cfg()\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
        "\n",
        "        # ROI HEADS SCORE THRESHOLD\n",
        "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "        # Comment the next line if you're using 'cuda'\n",
        "        # cfg['MODEL']['DEVICE']='cpu'\n",
        "\n",
        "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
        "\n",
        "        return cfg\n",
        "\n",
        "    def get_model(self, cfg):\n",
        "        # build model\n",
        "        model = build_model(cfg)\n",
        "\n",
        "        # load weights\n",
        "        checkpointer = DetectionCheckpointer(model)\n",
        "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "        # eval mode\n",
        "        model.eval()\n",
        "        return model        \n",
        "\n",
        "    def prepare_image_inputs(self, img_list):\n",
        "\n",
        "        #get model's cfg\n",
        "        cfg = self.cfg \n",
        "\n",
        "        # Resizing the image according to the configuration\n",
        "        transform_gen = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
        "        img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
        "\n",
        "        # Convert to C,H,W format\n",
        "        convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
        "\n",
        "        batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
        "\n",
        "        # Normalizing the image\n",
        "        num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
        "        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
        "        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
        "        normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
        "        images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
        "\n",
        "        # Convert to ImageList\n",
        "        images =  ImageList.from_tensors(images,self.model.backbone.size_divisibility)\n",
        "        \n",
        "        return images, batched_inputs\n",
        "\n",
        "    def get_visual_embeddings(self, img):\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     if (self.is_vision_model_loaded == False):\n",
        "        #         print(\"Loading CNN\\n\")\n",
        "        #         cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "        #         self.cnn = PretrainedCNN(cfg_path)      \n",
        "        #         # self.cfg = cfg_path\n",
        "        #         # self.model = PretrainedCNN.get_model(self.cfg)\n",
        "        #         self.is_vision_model_loaded = True\n",
        "\n",
        "        cfg = self.cfg\n",
        "        images, batched_inputs = self.prepare_image_inputs(img)\n",
        "        \n",
        "        features = self.model.backbone(images.tensor.cuda())\n",
        "        proposals, _ = self.model.proposal_generator(images, features)\n",
        "        \n",
        "        features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
        "        return features\n",
        "        #     box_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "        #     box_features = self.model.roi_heads.box_head.flatten(box_features)\n",
        "        #     box_features = self.model.roi_heads.box_head.fc1(box_features)\n",
        "        #     box_features = self.model.roi_heads.box_head.fc_relu1(box_features)\n",
        "        #     box_features = self.model.roi_heads.box_head.fc2(box_features)\n",
        "        #     # print (box_features.shape)\n",
        "        #     box_features = box_features.reshape(1, -1, 1024) # depends on your config and batch size\n",
        "        #     # box_features = box_features.reshape(1, -1, 2048) # depends on your config and batch size\n",
        "        \n",
        "        #     cls_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "        #     cls_features = self.model.roi_heads.box_head(cls_features)\n",
        "        #     pred_class_logits, pred_proposal_deltas = self.model.roi_heads.box_predictor(cls_features)\n",
        "\n",
        "        #     box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
        "        #     smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
        "\n",
        "        #     outputs = FastRCNNOutputs(\n",
        "        #         box2box_transform,\n",
        "        #         pred_class_logits,\n",
        "        #         pred_proposal_deltas,\n",
        "        #         proposals,\n",
        "        #         smooth_l1_beta,\n",
        "        #     )\n",
        "\n",
        "        #     boxes = outputs.predict_boxes()\n",
        "        #     scores = outputs.predict_probs()\n",
        "        #     image_shapes = outputs.image_shapes\n",
        "\n",
        "        #     output_boxes = [self.get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
        "\n",
        "        #     temp = [self.select_boxes(output_boxes[i], scores[i]) for i in range(len(scores))]\n",
        "\n",
        "        #     keep_boxes, max_conf = [],[]\n",
        "        #     for keep_box, mx_conf in temp:\n",
        "        #         keep_boxes.append(keep_box)\n",
        "        #         max_conf.append(mx_conf)\n",
        "\n",
        "        #     MIN_BOXES=10\n",
        "        #     MAX_BOXES=100\n",
        "\n",
        "        #     keep_boxes = [self.filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
        "        #     visual_embeds = [ box_feature[keep_box.copy()] for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
        "\n",
        "        # return visual_embeds\n",
        "\n",
        "    def get_output_boxes(self, boxes, batched_inputs, image_size):\n",
        "        proposal_boxes = boxes.reshape(-1, 4).clone()\n",
        "        scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
        "        output_boxes = Boxes(proposal_boxes)\n",
        "\n",
        "        output_boxes.scale(scale_x, scale_y)\n",
        "        output_boxes.clip(image_size)\n",
        "\n",
        "        return output_boxes\n",
        "\n",
        "    def filter_boxes(self, keep_boxes, max_conf, min_boxes, max_boxes):\n",
        "        \n",
        "        keep_boxes = keep_boxes.cpu()\n",
        "        max_conf = max_conf.cpu()\n",
        "\n",
        "        if len(keep_boxes) < min_boxes:\n",
        "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
        "        elif len(keep_boxes) > max_boxes:\n",
        "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
        "        return keep_boxes\n",
        "\n",
        "\n",
        "    def select_boxes(self, output_boxes, scores):\n",
        "\n",
        "        cfg = self.cnn.cfg\n",
        "        test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
        "        test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
        "        cls_prob = scores.detach()\n",
        "        # print (output_boxes.shape)\n",
        "        cls_boxes = output_boxes.tensor.detach().reshape(-1,80,4)\n",
        "        # cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
        "        max_conf = torch.zeros((cls_boxes.shape[0])).to(torch.device(\"cuda:0\"))\n",
        "        for cls_ind in range(0, cls_prob.shape[1]-1):\n",
        "            cls_scores = cls_prob[:, cls_ind+1]\n",
        "            det_boxes = cls_boxes[:,cls_ind,:]\n",
        "            keep = torch.from_numpy(np.array(nms(det_boxes, cls_scores, test_nms_thresh).cpu())).to(torch.device(\"cuda:0\"))\n",
        "            max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
        "        keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
        "        return keep_boxes, max_conf\n",
        "\n",
        "    def generate_visual_embeddings(self, data, split='train'):\n",
        "\n",
        "        img_folder = {'train':path+'images/train/', 'test':path+'images/test/', 'val':path+'images/val/'}\n",
        "        img_fmt = '.jpg'\n",
        "\n",
        "        train = data\n",
        "        i = np.random.randint(0,len(data),1)[0]\n",
        "        print (i)        \n",
        "        img_path = img_folder['train'] + train[i][0] + img_fmt\n",
        "        img = cv2.imread(img_path)\n",
        "        visual_embeds = self.get_visual_embeddings([img])\n",
        "        print (\"returning\")\n",
        "        return img, visual_embeds\n",
        "\n",
        "        if (split=='train'):\n",
        "            train = data\n",
        "            for i in range(len(train)):    \n",
        "                if (train[i][0] not in []):\n",
        "                    img_path = img_folder['train'] + train[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    print (\"returning\")\n",
        "                    return img, visual_embeds\n",
        "                    print (\"Now?\")\n",
        "                    self.visual_embeddings[train[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/train_img_features.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "            \n",
        "        elif (split=='test'):\n",
        "            test = data\n",
        "            self.visual_embeddings = {}\n",
        "            for i in range(len(test)):    \n",
        "                if (test[i][0] not in self.visual_embeddings):\n",
        "                    img_path = img_folder['test'] + test[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    self.visual_embeddings[test[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/img_features_test.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "        \n",
        "        else:\n",
        "            self.visual_embeddings = {}\n",
        "            for i in range(len(val)):    \n",
        "                if (val[i][0] not in self.visual_embeddings):\n",
        "                    img_path = img_folder['val'] + val[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    self.visual_embeddings[val[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/img_features_val.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "\n",
        "\n",
        "'''\n",
        "    Extract visual embeddings and save in pickle file\n",
        "    DONT RUN AGAIN UNLESS NEEDED\n",
        "'''\n",
        "\n",
        "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "cnn = PretrainedCNN(cfg_path)\n",
        "\n",
        "# path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "# train, test, val, ans2label = load_data(path)\n",
        "# print (\"Generating visual embeddings\")\n",
        "# cnn.generate_visual_embeddings('train')\n",
        "# cnn.generate_visual_embeddings('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u91UtDKM5D84",
        "outputId": "60321343-8c2a-4406-91ad-7ea778918fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_a3ec72.pkl: 254MB [00:22, 11.2MB/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img_path = '/content/drive/MyDrive/CS685/project/plots_and_figures/'\n",
        "\n",
        "def viz_features_from_image(url_path, nbr):\n",
        "    req = urllib.request.urlopen(url_path)\n",
        "    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
        "    img = cv2.imdecode(arr, -1) # 'Load it as it is'\n",
        "    disp_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(disp_img)\n",
        "    plt.savefig(img_path + f'coco_example_{nbr}.jpg')\n",
        "\n",
        "    features = cnn.get_visual_embeddings([disp_img])\n",
        "\n",
        "    it = 1\n",
        "    for key in features.keys():\n",
        "        if (key == 'p5' or key == 'p6'):\n",
        "            plt.imshow(features[key][0,0,:,:].squeeze().cpu().detach().numpy(), cmap='jet')\n",
        "            # plt.savefig(img_path + f'coco_example_features_{nbr}_{it}.jpg')\n",
        "            plt.show()\n",
        "            it += 1\n",
        "\n",
        "def viz_features_from_dataset(nbr):\n",
        "\n",
        "    #returns features for a random example in the training set\n",
        "    img2, features2 = cnn.generate_visual_embeddings(train, 'train')\n",
        "    disp_img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(disp_img2)\n",
        "    plt.savefig(img_path + f'pathvqa_example_{nbr}.jpg')\n",
        "\n",
        "    it = 1\n",
        "    for key in features2.keys():\n",
        "        if (key == 'p5' or key == 'p6'):\n",
        "            plt.imshow(features2[key][0,0,:,:].squeeze().cpu().detach().numpy(), cmap='jet')\n",
        "            # plt.savefig(img_path + f'pathvqa_example_features_{nbr}_{it}.jpg')\n",
        "            plt.show()\n",
        "            it += 1\n",
        "\n",
        "viz_features_from_image(\"https://farm4.staticflickr.com/3539/3634290064_8e2ded65e3_z.jpg\", 1)\n",
        "viz_features_from_image(\"https://farm8.staticflickr.com/7003/6649994945_c5e92895f7_z.jpg\", 2)\n",
        "viz_features_from_dataset(1)\n",
        "viz_features_from_dataset(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "TQfTUHs97uCa",
        "outputId": "95d04fb5-78b5-476e-cfcf-5aaa777042ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d5641ca27c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mviz_features_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://farm4.staticflickr.com/3539/3634290064_8e2ded65e3_z.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mviz_features_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://farm8.staticflickr.com/7003/6649994945_c5e92895f7_z.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mviz_features_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2d5641ca27c7>\u001b[0m in \u001b[0;36mviz_features_from_image\u001b[0;34m(url_path, nbr)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviz_features_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 'Load it as it is'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'request'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting class distribution for training set\n",
        "\n",
        "Plot a histogram showing the frequency of 100 most frequent classes"
      ],
      "metadata": {
        "id": "5LdYBiih0OHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive \n",
        "\n",
        "train_path = '/content/drive/MyDrive/PathVQA/split/qas/'\n",
        "train_vqa = pd.read_pickle(train_path + \"train_vqa.pkl\")\n",
        "train_qa = pd.read_pickle(train_path + \"train/train_qa.pkl\")\n",
        "\n",
        "answers = {}\n",
        "\n",
        "for example in train_qa:\n",
        "\n",
        "    if (example['answer'] not in answers):\n",
        "        answers[example['answer']] = 1\n",
        "    else:\n",
        "        answers[example['answer']] += 1\n",
        "\n",
        "answer_types = {}\n",
        "question_types = {}\n",
        "\n",
        "for example in train_vqa:\n",
        "\n",
        "    if (example['answer_type'] not in answer_types):\n",
        "        answer_types[example['answer_type']] = 1\n",
        "    else:\n",
        "        answer_types[example['answer_type']] += 1\n",
        "\n",
        "    if (example['question_type'] not in question_types):\n",
        "        question_types[example['question_type']] = 1\n",
        "    else:\n",
        "        question_types[example['question_type']] += 1\n",
        "\n",
        "cnt = []\n",
        "for answer in answers:\n",
        "    if answers[answer] >= 10:\n",
        "        cnt.append([answer, int(answers[answer])])"
      ],
      "metadata": {
        "id": "DDzqJ2hz0NIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.asarray(cnt)[:, 0]\n",
        "y = np.asarray(cnt)[:, 1]\n",
        "y = [int(element) for element in y]\n",
        "\n",
        "sorted_lists = sorted(zip(y,x),reverse=True)\n",
        "tuples = zip(*sorted_lists)\n",
        "y,x = [ list(tuple) for tuple in  tuples]\n",
        "\n",
        "plt.bar(np.arange(len(y)), height=y)\n",
        "# plt.savefig('/content/drive/MyDrive/CS685/project/plots_and_figures/training_dist.jpg')\n",
        "plt.show()\n",
        "\n",
        "print (f'Dataset contains {len(answer_types)} types of answers')\n",
        "print (f'Dataset contains {len(question_types)} types of questions')\n",
        "print (answer_types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "-hZp9YoK4oNG",
        "outputId": "41b5ed61-fe72-41a0-c5ac-38f31bc6f734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 3 types of answers\n",
            "Dataset contains 26 types of questions\n",
            "{'other': 9887, 'yes/no': 9806, 'number': 62}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get test set predictions"
      ],
      "metadata": {
        "id": "-0bhLbWhFqpB"
      }
    }
  ]
}