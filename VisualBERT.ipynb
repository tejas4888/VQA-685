{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisualBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X0r93dQIp-8o",
        "3CpMK7XwqVIi",
        "q_HfkFFPtWDs",
        "nm5lLiL41sZX",
        "IaG3PassXyht",
        "8diSc0rCOYwr",
        "wSLC9YwpXtsE"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac73f2de999448f0bb144aa856558f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b81369fd32bd49a9b2b1bc38ba5a83b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78ef7943f30741cf94c29b7987d2f185",
              "IPY_MODEL_dab2f740df734210b20837dccaf591b1",
              "IPY_MODEL_288d91d44a88475d95f9d97f8ca23b3b"
            ]
          }
        },
        "b81369fd32bd49a9b2b1bc38ba5a83b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78ef7943f30741cf94c29b7987d2f185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a940648bf97e4b4ab64d0217b752786f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12ba5b06606e48fdbad651a237cdc283"
          }
        },
        "dab2f740df734210b20837dccaf591b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43f153c2b74c4dd0b92776c57d0a2aab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 631,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 631,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_407b2509dea140af891dcece39a7716d"
          }
        },
        "288d91d44a88475d95f9d97f8ca23b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5222d1f5cb914444855602faeb7c82a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 631/631 [00:00&lt;00:00, 14.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43be34c52b3f46449322ddd0a4ea1c82"
          }
        },
        "a940648bf97e4b4ab64d0217b752786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12ba5b06606e48fdbad651a237cdc283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43f153c2b74c4dd0b92776c57d0a2aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "407b2509dea140af891dcece39a7716d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5222d1f5cb914444855602faeb7c82a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43be34c52b3f46449322ddd0a4ea1c82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90a1fce82d6e425da5e2b4650f4287a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6908fc964134aaaa2ee0f6aa283519d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1f5f6cd02dc84ef4a272bcaa40189fc5",
              "IPY_MODEL_e42980c66ce849c3b0ded48c2ebba5d9",
              "IPY_MODEL_7181d7c731cb4b71924e43fe1135aa15"
            ]
          }
        },
        "d6908fc964134aaaa2ee0f6aa283519d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f5f6cd02dc84ef4a272bcaa40189fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5cf299a9e49747208a92b2f41990916d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5672465f535d47d3ab15888c5ff040b5"
          }
        },
        "e42980c66ce849c3b0ded48c2ebba5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_98c211afa4b24033912613457f13aa46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 445210461,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 445210461,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e51d3c50f194c059f849d689d79b294"
          }
        },
        "7181d7c731cb4b71924e43fe1135aa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31bacdc6b8824c2eb96c721a5201caa5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 425M/425M [00:22&lt;00:00, 33.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a363a31e630467aa9108906d6005349"
          }
        },
        "5cf299a9e49747208a92b2f41990916d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5672465f535d47d3ab15888c5ff040b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98c211afa4b24033912613457f13aa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e51d3c50f194c059f849d689d79b294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31bacdc6b8824c2eb96c721a5201caa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a363a31e630467aa9108906d6005349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "213482ab902641399307d1596beccd49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b2c1d4da92d84130839d63d70addf0e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d72dba0ce3ea43fe841b95e4bc335e16",
              "IPY_MODEL_524365c5d452400faea4e4dfef8985e1",
              "IPY_MODEL_2e165baa6c654649b560a527afa0b258"
            ]
          }
        },
        "b2c1d4da92d84130839d63d70addf0e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d72dba0ce3ea43fe841b95e4bc335e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f32bc41eb4be49d9b5576d983119ab08",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b8aa1e8e0bd47b090523a94d64d5f3d"
          }
        },
        "524365c5d452400faea4e4dfef8985e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cbec7e61e9ea46c6878450f81e575be3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b21609097434fcd9b12fd5d1ab65da9"
          }
        },
        "2e165baa6c654649b560a527afa0b258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0edc096fc9a49098a4e716738c8d739",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 911kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96d0d808e5224bdbbdeef0be5c1d65f4"
          }
        },
        "f32bc41eb4be49d9b5576d983119ab08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b8aa1e8e0bd47b090523a94d64d5f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbec7e61e9ea46c6878450f81e575be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b21609097434fcd9b12fd5d1ab65da9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0edc096fc9a49098a4e716738c8d739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96d0d808e5224bdbbdeef0be5c1d65f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f60a15c1a2a4d48bfa942c9c16ea85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_385896af6d864adbb4c901d96cedf06a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cce1aeb99e2a4648a8828f1211b9302e",
              "IPY_MODEL_ad58c81aedc44b1aa0e1d5fe0895507b",
              "IPY_MODEL_3e0c87cd3ff841e1bed0a35aaf1e0ea2"
            ]
          }
        },
        "385896af6d864adbb4c901d96cedf06a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cce1aeb99e2a4648a8828f1211b9302e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59e601e114414a5ca827b8118d76e7ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10c29a5df94a4549a40392015dc8228e"
          }
        },
        "ad58c81aedc44b1aa0e1d5fe0895507b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_348431e779ea42a09a4062d74940f460",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1147fde3aaf049e08ecd402f19ac151d"
          }
        },
        "3e0c87cd3ff841e1bed0a35aaf1e0ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f362fb3b948d41ed830f837529f25a23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 696B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cf7015105e343d7b98046b5ae0ee4f1"
          }
        },
        "59e601e114414a5ca827b8118d76e7ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10c29a5df94a4549a40392015dc8228e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "348431e779ea42a09a4062d74940f460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1147fde3aaf049e08ecd402f19ac151d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f362fb3b948d41ed830f837529f25a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cf7015105e343d7b98046b5ae0ee4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ad45aa2c54a408da7b3ecc7513774d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3824419b611047bdb935866f4f2bd29c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de81c8dd2c8345f9bc42c40539830eb1",
              "IPY_MODEL_231b62568aef49458d807aef909b7332",
              "IPY_MODEL_83235179f394421cae09ee2355795a26"
            ]
          }
        },
        "3824419b611047bdb935866f4f2bd29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de81c8dd2c8345f9bc42c40539830eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4621427fc443434ba1a175aa1e86690e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c07c47407784238a7e94ba71ab4de2c"
          }
        },
        "231b62568aef49458d807aef909b7332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8af3f61b20104c06850e55c836280ef1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_772edd7d7d77428e8211990f9b6f3503"
          }
        },
        "83235179f394421cae09ee2355795a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc759dda77c1437c9d2982b7e33b35c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 455k/455k [00:00&lt;00:00, 889kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5bec50bde594e3fbeb8888c4a326ced"
          }
        },
        "4621427fc443434ba1a175aa1e86690e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c07c47407784238a7e94ba71ab4de2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8af3f61b20104c06850e55c836280ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "772edd7d7d77428e8211990f9b6f3503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc759dda77c1437c9d2982b7e33b35c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5bec50bde594e3fbeb8888c4a326ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "275874e139d943e88c597e79c39821d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba09a5f9b0e141a6a522cfe0eacff617",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_24f02218f23b4a29a722b14d2dea623e",
              "IPY_MODEL_82c5d61460eb42028bc076dabb271045",
              "IPY_MODEL_d9e41b33ae184f6592d37249a1606111"
            ]
          }
        },
        "ba09a5f9b0e141a6a522cfe0eacff617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24f02218f23b4a29a722b14d2dea623e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1e0f09b2f624b0aa6d41d804ff33073",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44378193b7db4dcba3324d82bd291b97"
          }
        },
        "82c5d61460eb42028bc076dabb271045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_290e05050ec64fa88b1b96bc7631d6c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92332fc5a4a647f988151033610397ae"
          }
        },
        "d9e41b33ae184f6592d37249a1606111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3df474e0067249d789f2190e39d4c623",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 15.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f73b05dbeef46a98cd9c9e20ed8e849"
          }
        },
        "a1e0f09b2f624b0aa6d41d804ff33073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44378193b7db4dcba3324d82bd291b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "290e05050ec64fa88b1b96bc7631d6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92332fc5a4a647f988151033610397ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3df474e0067249d789f2190e39d4c623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f73b05dbeef46a98cd9c9e20ed8e849": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas4888/VQA-685/blob/main/VisualBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "X0r93dQIp-8o"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKTm7HeIuzNM"
      },
      "source": [
        "## code from Detectron library\n",
        "## added due to compatibility issues\n",
        "\n",
        "class FastRCNNOutputs:\n",
        "    \"\"\"\n",
        "    An internal implementation that stores information about outputs of a Fast R-CNN head,\n",
        "    and provides methods that are used to decode the outputs of a Fast R-CNN head.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        box2box_transform,\n",
        "        pred_class_logits,\n",
        "        pred_proposal_deltas,\n",
        "        proposals,\n",
        "        smooth_l1_beta=0.0,\n",
        "        box_reg_loss_type=\"smooth_l1\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            box2box_transform (Box2BoxTransform/Box2BoxTransformRotated):\n",
        "                box2box transform instance for proposal-to-detection transformations.\n",
        "            pred_class_logits (Tensor): A tensor of shape (R, K + 1) storing the predicted class\n",
        "                logits for all R predicted object instances.\n",
        "                Each row corresponds to a predicted object instance.\n",
        "            pred_proposal_deltas (Tensor): A tensor of shape (R, K * B) or (R, B) for\n",
        "                class-specific or class-agnostic regression. It stores the predicted deltas that\n",
        "                transform proposals into final box detections.\n",
        "                B is the box dimension (4 or 5).\n",
        "                When B is 4, each row is [dx, dy, dw, dh (, ....)].\n",
        "                When B is 5, each row is [dx, dy, dw, dh, da (, ....)].\n",
        "            proposals (list[Instances]): A list of N Instances, where Instances i stores the\n",
        "                proposals for image i, in the field \"proposal_boxes\".\n",
        "                When training, each Instances must have ground-truth labels\n",
        "                stored in the field \"gt_classes\" and \"gt_boxes\".\n",
        "                The total number of all instances must be equal to R.\n",
        "            smooth_l1_beta (float): The transition point between L1 and L2 loss in\n",
        "                the smooth L1 loss function. When set to 0, the loss becomes L1. When\n",
        "                set to +inf, the loss becomes constant 0.\n",
        "            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n",
        "        \"\"\"\n",
        "        self.box2box_transform = box2box_transform\n",
        "        self.num_preds_per_image = [len(p) for p in proposals]\n",
        "        self.pred_class_logits = pred_class_logits\n",
        "        self.pred_proposal_deltas = pred_proposal_deltas\n",
        "        self.smooth_l1_beta = smooth_l1_beta\n",
        "        self.box_reg_loss_type = box_reg_loss_type\n",
        "\n",
        "        self.image_shapes = [x.image_size for x in proposals]\n",
        "\n",
        "        if len(proposals):\n",
        "            box_type = type(proposals[0].proposal_boxes)\n",
        "            # cat(..., dim=0) concatenates over all images in the batch\n",
        "            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])\n",
        "            assert (\n",
        "                not self.proposals.tensor.requires_grad\n",
        "            ), \"Proposals should not require gradients!\"\n",
        "\n",
        "            # \"gt_classes\" exists if and only if training. But other gt fields may\n",
        "            # not necessarily exist in training for images that have no groundtruth.\n",
        "            if proposals[0].has(\"gt_classes\"):\n",
        "                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n",
        "\n",
        "                # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
        "                # should not be included in regression loss computation.\n",
        "                # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
        "                # value won't be used in self.box_reg_loss().\n",
        "                gt_boxes = [\n",
        "                    p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes for p in proposals\n",
        "                ]\n",
        "                self.gt_boxes = box_type.cat(gt_boxes)\n",
        "        else:\n",
        "            self.proposals = Boxes(torch.zeros(0, 4, device=self.pred_proposal_deltas.device))\n",
        "        self._no_instances = len(self.proposals) == 0  # no instances found\n",
        "\n",
        "    def softmax_cross_entropy_loss(self):\n",
        "        \"\"\"\n",
        "        Deprecated\n",
        "        \"\"\"\n",
        "        _log_classification_stats(self.pred_class_logits, self.gt_classes)\n",
        "        return cross_entropy(self.pred_class_logits, self.gt_classes, reduction=\"mean\")\n",
        "\n",
        "    def box_reg_loss(self):\n",
        "        \"\"\"\n",
        "        Deprecated\n",
        "        \"\"\"\n",
        "        if self._no_instances:\n",
        "            return 0.0 * self.pred_proposal_deltas.sum()\n",
        "\n",
        "        box_dim = self.proposals.tensor.size(1)  # 4 or 5\n",
        "        cls_agnostic_bbox_reg = self.pred_proposal_deltas.size(1) == box_dim\n",
        "        device = self.pred_proposal_deltas.device\n",
        "\n",
        "        bg_class_ind = self.pred_class_logits.shape[1] - 1\n",
        "        # Box delta loss is only computed between the prediction for the gt class k\n",
        "        # (if 0 <= k < bg_class_ind) and the target; there is no loss defined on predictions\n",
        "        # for non-gt classes and background.\n",
        "        # Empty fg_inds should produce a valid loss of zero because reduction=sum.\n",
        "        fg_inds = nonzero_tuple((self.gt_classes >= 0) & (self.gt_classes < bg_class_ind))[0]\n",
        "\n",
        "        if cls_agnostic_bbox_reg:\n",
        "            # pred_proposal_deltas only corresponds to foreground class for agnostic\n",
        "            gt_class_cols = torch.arange(box_dim, device=device)\n",
        "        else:\n",
        "            # pred_proposal_deltas for class k are located in columns [b * k : b * k + b],\n",
        "            # where b is the dimension of box representation (4 or 5)\n",
        "            # Note that compared to Detectron1,\n",
        "            # we do not perform bounding box regression for background classes.\n",
        "            gt_class_cols = box_dim * self.gt_classes[fg_inds, None] + torch.arange(\n",
        "                box_dim, device=device\n",
        "            )\n",
        "\n",
        "        if self.box_reg_loss_type == \"smooth_l1\":\n",
        "            gt_proposal_deltas = self.box2box_transform.get_deltas(\n",
        "                self.proposals.tensor, self.gt_boxes.tensor\n",
        "            )\n",
        "            loss_box_reg = smooth_l1_loss(\n",
        "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
        "                gt_proposal_deltas[fg_inds],\n",
        "                self.smooth_l1_beta,\n",
        "                reduction=\"sum\",\n",
        "            )\n",
        "        elif self.box_reg_loss_type == \"giou\":\n",
        "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
        "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
        "                self.proposals.tensor[fg_inds],\n",
        "            )\n",
        "            loss_box_reg = giou_loss(\n",
        "                fg_pred_boxes,\n",
        "                self.gt_boxes.tensor[fg_inds],\n",
        "                reduction=\"sum\",\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n",
        "\n",
        "        loss_box_reg = loss_box_reg / self.gt_classes.numel()\n",
        "        return loss_box_reg\n",
        "\n",
        "    def losses(self):\n",
        "        \"\"\"\n",
        "        Deprecated\n",
        "        \"\"\"\n",
        "        return {\"loss_cls\": self.softmax_cross_entropy_loss(), \"loss_box_reg\": self.box_reg_loss()}\n",
        "\n",
        "    def predict_boxes(self):\n",
        "        \"\"\"\n",
        "        Deprecated\n",
        "        \"\"\"\n",
        "        pred = self.box2box_transform.apply_deltas(self.pred_proposal_deltas, self.proposals.tensor)\n",
        "        return pred.split(self.num_preds_per_image, dim=0)\n",
        "\n",
        "    def predict_probs(self):\n",
        "        \"\"\"\n",
        "        Deprecated\n",
        "        \"\"\"\n",
        "        probs = F.softmax(self.pred_class_logits, dim=-1)\n",
        "        return probs.split(self.num_preds_per_image, dim=0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7fw9xN2StE6",
        "outputId": "05cd732e-f752-4021-a4e6-94f6c23afe81"
      },
      "source": [
        "!pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 274 kB 8.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=8e7858de76216973692cd20aec9f546564a8d1fd413e3e97426afdb35d6e6121\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-p_w7virp\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-p_w7virp\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.7.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 43.3 MB/s \n",
            "\u001b[?25hCollecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n",
            "Collecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (3.10.0.2)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n",
            "Collecting regex>=2020.1.8\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.4.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (0.29.24)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.6.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.1.1)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp37-cp37m-linux_x86_64.whl size=5723042 sha256=479819f33854411ac017b6e3f6bc8775203bab0259e0689eb9a12128bda20485\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v8nygdeb/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=a612844c0c9f0a1ae033bb6234e40d6a8b98e487bf555b54b57ea801770f9106\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=27c84f8f038b3a8795e0759336e13bbdcb5ea40ad6d8358406126adc6b29235d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20211023 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.3.2 regex-2021.11.10 typed-ast-1.5.1 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUwzRBr-AAKw",
        "outputId": "649aea93-5ebc-4cd6-aa88-145de67ddc2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 495 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF_E_tTsPJnC"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "import torch, torchvision\n",
        "import yaml\n",
        "import json \n",
        "import cv2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8EUjamjkT6p"
      },
      "source": [
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.structures.image_list import ImageList\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.modeling.box_regression import Box2BoxTransform\n",
        "# from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
        "from detectron2.structures.boxes import Boxes\n",
        "from detectron2.layers import nms\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model to extract visual embeddings"
      ],
      "metadata": {
        "id": "3CpMK7XwqVIi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90Wt1xbzVXUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "079e1df5-9025-4700-a708-3c90aac1365b"
      },
      "source": [
        "class PretrainedCNN:\n",
        "\n",
        "    def __init__(self, cfg_path):\n",
        "\n",
        "        self.cfg = self.load_config_and_model_weights(cfg_path)\n",
        "        self.model = self.get_model(self.cfg)\n",
        "\n",
        "    def load_config_and_model_weights(self, cfg_path):\n",
        "        cfg = get_cfg()\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
        "\n",
        "        # ROI HEADS SCORE THRESHOLD\n",
        "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "        # Comment the next line if you're using 'cuda'\n",
        "        # cfg['MODEL']['DEVICE']='cpu'\n",
        "\n",
        "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
        "\n",
        "        return cfg\n",
        "\n",
        "    def get_model(self, cfg):\n",
        "        # build model\n",
        "        model = build_model(cfg)\n",
        "\n",
        "        # load weights\n",
        "        checkpointer = DetectionCheckpointer(model)\n",
        "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
        "\n",
        "        # eval mode\n",
        "        model.eval()\n",
        "        return model        \n",
        "\n",
        "    def prepare_image_inputs(self, img_list):\n",
        "\n",
        "        #get model's cfg\n",
        "        cfg = self.cfg \n",
        "\n",
        "        # Resizing the image according to the configuration\n",
        "        transform_gen = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n",
        "        img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
        "\n",
        "        # Convert to C,H,W format\n",
        "        convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
        "\n",
        "        batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
        "\n",
        "        # Normalizing the image\n",
        "        num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
        "        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
        "        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
        "        normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
        "        images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
        "\n",
        "        # Convert to ImageList\n",
        "        images =  ImageList.from_tensors(images,self.model.backbone.size_divisibility)\n",
        "        \n",
        "        return images, batched_inputs\n",
        "\n",
        "        def get_visual_embeddings(self, img):\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if (self.is_vision_model_loaded == False):\n",
        "                    print(\"Loading CNN\\n\")\n",
        "                    cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "                    self.cnn = PretrainedCNN(cfg_path)      \n",
        "                    # self.cfg = cfg_path\n",
        "                    # self.model = PretrainedCNN.get_model(self.cfg)\n",
        "                    self.is_vision_model_loaded = True\n",
        "\n",
        "            cfg = self.cfg\n",
        "            images, batched_inputs = self.prepare_image_inputs(img)\n",
        "            \n",
        "            features = self.model.backbone(images.tensor.cuda())\n",
        "            proposals, _ = self.model.proposal_generator(images, features)\n",
        "            \n",
        "            features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
        "            box_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "            box_features = self.model.roi_heads.box_head.flatten(box_features)\n",
        "            box_features = self.model.roi_heads.box_head.fc1(box_features)\n",
        "            box_features = self.model.roi_heads.box_head.fc_relu1(box_features)\n",
        "            box_features = self.model.roi_heads.box_head.fc2(box_features)\n",
        "            # print (box_features.shape)\n",
        "            box_features = box_features.reshape(1, -1, 1024) # depends on your config and batch size\n",
        "            # box_features = box_features.reshape(1, -1, 2048) # depends on your config and batch size\n",
        "        \n",
        "            cls_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
        "            cls_features = self.model.roi_heads.box_head(cls_features)\n",
        "            pred_class_logits, pred_proposal_deltas = self.model.roi_heads.box_predictor(cls_features)\n",
        "\n",
        "            box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
        "            smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
        "\n",
        "            outputs = FastRCNNOutputs(\n",
        "                box2box_transform,\n",
        "                pred_class_logits,\n",
        "                pred_proposal_deltas,\n",
        "                proposals,\n",
        "                smooth_l1_beta,\n",
        "            )\n",
        "\n",
        "            boxes = outputs.predict_boxes()\n",
        "            scores = outputs.predict_probs()\n",
        "            image_shapes = outputs.image_shapes\n",
        "\n",
        "            output_boxes = [self.get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
        "\n",
        "            temp = [self.select_boxes(output_boxes[i], scores[i]) for i in range(len(scores))]\n",
        "\n",
        "            keep_boxes, max_conf = [],[]\n",
        "            for keep_box, mx_conf in temp:\n",
        "                keep_boxes.append(keep_box)\n",
        "                max_conf.append(mx_conf)\n",
        "\n",
        "            MIN_BOXES=10\n",
        "            MAX_BOXES=100\n",
        "\n",
        "            keep_boxes = [self.filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
        "            visual_embeds = [ box_feature[keep_box.copy()] for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
        "\n",
        "        return visual_embeds\n",
        "\n",
        "    def get_output_boxes(self, boxes, batched_inputs, image_size):\n",
        "        proposal_boxes = boxes.reshape(-1, 4).clone()\n",
        "        scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
        "        output_boxes = Boxes(proposal_boxes)\n",
        "\n",
        "        output_boxes.scale(scale_x, scale_y)\n",
        "        output_boxes.clip(image_size)\n",
        "\n",
        "        return output_boxes\n",
        "\n",
        "    def filter_boxes(self, keep_boxes, max_conf, min_boxes, max_boxes):\n",
        "        \n",
        "        keep_boxes = keep_boxes.cpu()\n",
        "        max_conf = max_conf.cpu()\n",
        "\n",
        "        if len(keep_boxes) < min_boxes:\n",
        "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
        "        elif len(keep_boxes) > max_boxes:\n",
        "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
        "        return keep_boxes\n",
        "\n",
        "\n",
        "    def select_boxes(self, output_boxes, scores):\n",
        "\n",
        "        cfg = self.cnn.cfg\n",
        "        test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
        "        test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
        "        cls_prob = scores.detach()\n",
        "        # print (output_boxes.shape)\n",
        "        cls_boxes = output_boxes.tensor.detach().reshape(-1,80,4)\n",
        "        # cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
        "        max_conf = torch.zeros((cls_boxes.shape[0])).to(torch.device(\"cuda:0\"))\n",
        "        for cls_ind in range(0, cls_prob.shape[1]-1):\n",
        "            cls_scores = cls_prob[:, cls_ind+1]\n",
        "            det_boxes = cls_boxes[:,cls_ind,:]\n",
        "            keep = torch.from_numpy(np.array(nms(det_boxes, cls_scores, test_nms_thresh).cpu())).to(torch.device(\"cuda:0\"))\n",
        "            max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
        "        keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
        "        return keep_boxes, max_conf\n",
        "\n",
        "    def generate_visual_embeddings(self, data, split='train'):\n",
        "\n",
        "        img_folder = {'train':path+'images/train/', 'test':path+'images/test/', 'val':path+'images/val/'}\n",
        "        img_fmt = '.jpg'\n",
        "        \n",
        "        if (split=='train'):\n",
        "            train = data\n",
        "            for i in range(len(train)):    \n",
        "                if (train[i][0] not in self.visual_embeddings):\n",
        "                    img_path = img_folder['train'] + train[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    self.visual_embeddings[train[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/train_img_features.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "            \n",
        "        elif (split=='test'):\n",
        "            test = data\n",
        "            self.visual_embeddings = {}\n",
        "            for i in range(len(test)):    \n",
        "                if (test[i][0] not in self.visual_embeddings):\n",
        "                    img_path = img_folder['test'] + test[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    self.visual_embeddings[test[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/img_features_test.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "        \n",
        "        else:\n",
        "            self.visual_embeddings = {}\n",
        "            for i in range(len(val)):    \n",
        "                if (val[i][0] not in self.visual_embeddings):\n",
        "                    img_path = img_folder['val'] + val[i][0] + img_fmt\n",
        "                    img = cv2.imread(img_path)\n",
        "                    visual_embeds = self.get_visual_embeddings([img])\n",
        "                    self.visual_embeddings[val[i][0]] = visual_embeds\n",
        "            f = open(\"/content/drive/MyDrive/CS685/project/img_features_val.pkl\",\"wb\")\n",
        "            pickle.dump(self.visual_embeddings,f)\n",
        "            f.close()\n",
        "\n",
        "\n",
        "'''\n",
        "    Extract visual embeddings and save in pickle file\n",
        "    DONT RUN AGAIN UNLESS NEEDED\n",
        "'''\n",
        "\n",
        "cfg_path = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
        "cnn = PretrainedCNN(cfg_path)\n",
        "\n",
        "# path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "# train, test, val, ans2label = load_data(path)\n",
        "# print (\"Generating visual embeddings\")\n",
        "# cnn.generate_visual_embeddings(train, 'train')\n",
        "# cnn.generate_visual_embeddings(test, 'test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ec0714094120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0mcfg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;31m# path = \"/content/drive/MyDrive/PathVQA/split/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ec0714094120>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config_and_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ec0714094120>\u001b[0m in \u001b[0;36mload_config_and_model_weights\u001b[0;34m(self, cfg_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_config_and_model_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_cfg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and mount google drive\n",
        "\n",
        "Please create a shortcut in your Google Drive to this folder: **INSERT LINK**"
      ],
      "metadata": {
        "id": "q_HfkFFPtWDs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEUjRWj4WhY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5e38ac-519e-42d0-c5b4-7f5ec7a86d89"
      },
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import drive \n",
        "\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def load_data(path):\n",
        "\n",
        "    # path = \"/content/drive/MyDrive/PathVQA/split/\"    \n",
        "\n",
        "    train_path = path + 'qas/train/'\n",
        "    test_path = path + 'qas/test/'\n",
        "    val_path = path + 'qas/val/'\n",
        "\n",
        "    print (\"\\nLoading PathVQA\")\n",
        "    train_qa = pd.read_pickle(train_path + \"train_qa.pkl\")\n",
        "    test_qa = pd.read_pickle(test_path + \"test_qa.pkl\")\n",
        "    val_qa = pd.read_pickle(val_path + \"val_qa.pkl\")\n",
        "    ans2label = pd.read_pickle(path+'qas/ans2label.pkl')\n",
        "    \n",
        "    train = []\n",
        "    for row in train_qa:\n",
        "        if row['answer'] in ans2label:\n",
        "            train.append([ row['image'], row['question'], int(ans2label[row['answer']]) ] )\n",
        "\n",
        "    test = []\n",
        "    for row in test_qa:\n",
        "        if row['answer'] in ans2label:\n",
        "            test.append([row['image'], row['question'], int(ans2label[row['answer']]) ])\n",
        "\n",
        "    val = []\n",
        "    for row in val_qa:\n",
        "            if row['answer'] in ans2label:\n",
        "                val.append([row['image'], row['question'], int(ans2label[row['answer']]) ])\n",
        "\n",
        "    # train = np.array()\n",
        "\n",
        "    # return train_qa, test_qa, val_qa, ans2label\n",
        "    return np.asarray(train), np.asarray(test), np.asarray(val), ans2label\n",
        "\n",
        "mount_drive()\n",
        "path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "train, test, val, ans2label = load_data(path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Loading PathVQA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frozen VisualBERT\n",
        "\n",
        "Losses are backpropogated only through the classifier."
      ],
      "metadata": {
        "id": "nm5lLiL41sZX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY6uX8CYVQ_j"
      },
      "source": [
        "import pickle\n",
        "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertForPreTraining\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "\n",
        "        '''\n",
        "            Build a classification head\n",
        "        '''\n",
        "\n",
        "        super().__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dims, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 2048)\n",
        "        self.fc3 = torch.nn.Linear(2048, output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Model:\n",
        "\n",
        "    #take necessary inputs\n",
        "    #input_dims, output_dims, batch_size_test, batch_size_train\n",
        "    def __init__(self, output_dims, lr):\n",
        "\n",
        "        # self.model = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n",
        "        # model below takes vis embeds of dim 1024 \n",
        "        self.model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', output_hidden_states=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = self.model.cuda()\n",
        "\n",
        "        # load classifier model also \n",
        "        self.input_dims = 768\n",
        "        self.output_dims = output_dims\n",
        "\n",
        "        # tested with both a single layer classifier and \n",
        "        # a more deep classification head\n",
        "        # self.classifier = torch.nn.Linear(self.input_dims, self.output_dims)\n",
        "        self.classifier = Classifier(self.input_dims, self.output_dims)\n",
        "        self.loss =torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # updating only classifier head\n",
        "        # visualBERT weights are frozen\n",
        "        self.optimizer = torch.optim.Adam(self.classifier.parameters(), lr=lr)\n",
        "        \n",
        "        # self.is_vision_model_loaded = False\n",
        "        self.visual_embeddings = {}\n",
        "        self.visual_embeddings_train = {}\n",
        "        self.visual_embeddings_test = {}\n",
        "        \n",
        "        self.batch_size = 1\n",
        "        self.lr = lr\n",
        "\n",
        "    def load_visual_embeddings(self, path, split='train'):\n",
        "\n",
        "        if split=='train':\n",
        "            self.visual_embeddings_train = pd.read_pickle(path)\n",
        "        elif split=='test':\n",
        "            self.visual_embeddings_test = pd.read_pickle(path)\n",
        "\n",
        "    def make_prediction(self, img_id, question,split='train'):\n",
        "        '''\n",
        "            should be a list of imgs/ques\n",
        "        '''\n",
        "\n",
        "        tokens = self.tokenizer(question, padding='max_length', max_length=100)\n",
        "        input_ids = torch.tensor(tokens[\"input_ids\"]).cuda()#.unsqueeze(0)\n",
        "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).cuda()\n",
        "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).cuda()\n",
        "        # visual_embeds = torch.stack(self.get_visual_embeddings(img_id)).cuda()\n",
        "\n",
        "        if split=='train':\n",
        "            for id in img_id:\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_train[img_id[0]]).cuda()        \n",
        "        else:\n",
        "            for id in img_id:\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_test[img_id[0]]).cuda()        \n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def test(self, batch_size=2, load_path=None):\n",
        "\n",
        "        print (\"Evaluating model\")\n",
        "        num_batches = int(len(test)) #iterating one example at a time\n",
        "        test_loss = 0.0\n",
        "        total_correct = 0.0\n",
        "\n",
        "        #load weights from given checkpoint\n",
        "        if load_path is not None:\n",
        "            print (f'Loading from {load_path}')\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.classifier.load_state_dict(checkpt['classifier_model_state_dict'])\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        #eval mode\n",
        "        self.model.eval()\n",
        "        self.classifier.eval()\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            batch = test[i:i+batch_size]\n",
        "            imgs = batch[:,0]\n",
        "            questions = list(batch[:,1])\n",
        "            labels = batch[:,2]\n",
        "\n",
        "            if (imgs == []) or (questions == []) or (labels == []):\n",
        "                continue\n",
        "\n",
        "            pred = self.make_prediction(imgs, questions, 'test')\n",
        "            # Extracting [CLS] token representation\n",
        "            features = pred.hidden_states[11][0][0]\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                class_pred = self.classifier(features)\n",
        "                y_pred = torch.log_softmax(class_pred.unsqueeze(0), dim=1)\n",
        "                _, y_pred_tags = torch.max(y_pred, dim=1)\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "\n",
        "                if (int(labels[0]) == y_pred_tags[0].item()):\n",
        "                    total_correct+=1\n",
        "\n",
        "        print (f\"Test accuracy is {total_correct/len(test)} \\n\")\n",
        "        return total_correct/len(test)\n",
        "\n",
        "    def train(self, start_epoch=0, epochs=2, batch_size=1, load_path=None, save_path=None):\n",
        "\n",
        "        num_batches = int(len(train)) #iterate one example at a time\n",
        "        train_loss_log = []\n",
        "        test_acc_log = []\n",
        "        max_test = 0.0\n",
        "\n",
        "        print (\"Saving to: \", save_path)\n",
        "\n",
        "        self.model.train()\n",
        "        self.classifier.train()\n",
        "        \n",
        "        #load weights from given checkpoint\n",
        "        if load_path is not None:\n",
        "            print (load_path)\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.classifier.load_state_dict(checkpt['classifier_model_state_dict'])\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        for ep in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            self.classifier.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for i in range(num_batches):\n",
        "\n",
        "                assert(self.model.training and self.classifier.training)\n",
        "\n",
        "                if (i%4000==0 and i>0):\n",
        "                    print (f'Epoch {ep}, {i}/{num_batches} batches, loss is {train_loss/i}')\n",
        "                    # break\n",
        "\n",
        "                batch = train[i:i+self.batch_size]\n",
        "                imgs = batch[:,0]\n",
        "                questions = list(batch[:,1])\n",
        "                labels = batch[:,2]\n",
        "\n",
        "                if (imgs == []) or (questions == []) or (labels == []):\n",
        "                    continue\n",
        "\n",
        "                pred = self.make_prediction(imgs, questions, 'train')\n",
        "                features = pred.hidden_states[11][0][0]\n",
        "\n",
        "                class_pred = self.classifier(features).cuda()\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                loss = self.loss(class_pred.unsqueeze(0), gt.unsqueeze(0))\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "                #update weights for the batch\n",
        "                if ((i+1)%batch_size==0 or i==len(train)):\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    \n",
        "            (train_loss_log.append(train_loss))\n",
        "            \n",
        "            #save weights every 3 epochs\n",
        "            if (save_path != None and (ep+1)%3==0):\n",
        "                torch.save({\n",
        "                    'epoch': ep,\n",
        "                    'classifier_model_state_dict': self.classifier.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': train_loss_log,\n",
        "                    'vb_model_state_dict': self.model.state_dict(),\n",
        "                    'max_test': max_test,\n",
        "                    'test_acc_log': test_acc_log,\n",
        "                    'batch_size': batch_size,\n",
        "                    'lr' : self.lr\n",
        "                }, save_path+f\"/upgraded_b{batch_size}_lr{int(self.lr)}_{ep}.pth\")\n",
        "                print (\"Saved model to: \", save_path+f\"/upgraded_b{batch_size}_lr{int(10000*self.lr)}_{ep}.pth\")\n",
        "\n",
        "            \n",
        "            print (f'Completed {ep+1} epochs out of {epochs}, loss is {train_loss_log[ep]/len(train)} \\n')\n",
        "            test_acc = self.test(batch_size=1)\n",
        "\n",
        "            #compare test accuracy at this epoch, save best weights so far\n",
        "            try:\n",
        "                if (test_acc > max_test):\n",
        "                    max_test = test_acc\n",
        "                    torch.save({\n",
        "                        'epoch': ep,\n",
        "                        'classifier_model_state_dict': self.classifier.state_dict(),\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'loss': train_loss_log,\n",
        "                        'vb_model_state_dict': self.model.state_dict(),\n",
        "                        'max_test': max_test,\n",
        "                        'test_acc_log': test_acc_log,\n",
        "                        'batch_size': batch_size,\n",
        "                        'lr': self.lr\n",
        "                    }, save_path+f\"/upgraded_b{batch_size}_lr{int(self.lr)}_best.pth\")\n",
        "                    print (\"Saved model to: \", save_path+f\"/upgraded_b{batch_size}_lr{int(10000*self.lr)}_best.pth\")\n",
        "            except:\n",
        "                print (\"Could not check for the best model\")\n",
        "            "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZyNxcg0-PvC"
      },
      "source": [
        "# path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "# train, test, val, ans2label = load_data(path)\n",
        "# train_images, test_images, load_images(train, test, val)\n",
        "# classes = len(ans2label)\n",
        "# #output of BERT's lasthidden state is 768\n",
        "# feature_len = 768\n",
        "\n",
        "# Uncomment and run if not able to load pickle files\n",
        "# print (\"Loading model\")\n",
        "# visualbert = Model(len(ans2label))\n",
        "# visualbert.generate_visual_embeddings('test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Loading model\")\n",
        "visualbert = Model(len(ans2label), lr=0.001)\n",
        "\n",
        "#load visual embeddings from pickle file\n",
        "print (\"Loading visual embeddings\")\n",
        "visual_embeddings_path = \"/content/drive/MyDrive/CS685/project/img_features\"\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_train.pkl\", 'train')\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_test.pkl\", 'test')"
      ],
      "metadata": {
        "id": "GmDDPP9tGSb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DONT RUN WITH THE SAME PATH AS IT WILL OVERWRITE FILE\n",
        "\n",
        "load_path = None #'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\n",
        "visualbert.classifier.cuda()\n",
        "visualbert.train(epochs=20,batch_size=8,save_path='/content/drive/MyDrive/CS685/project/vb', load_path=load_path) "
      ],
      "metadata": {
        "id": "lzBpJBMtY92u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = None #'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\n",
        "visualbert.classifier.cuda()\n",
        "visualbert.test(batch_size=1, load_path=load_path)"
      ],
      "metadata": {
        "id": "T9jqUgOEHlEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfrozen VisualBERT\n",
        "\n",
        "Losses are backpropogated throughout the entire model.\n"
      ],
      "metadata": {
        "id": "hf_PetqJ1gkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertForPreTraining\n",
        "\n",
        "class CustomVB(torch.nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "            Integrates the classification head on top ob base VisualBERT\n",
        "            Loss is backpropogated throughout the model\n",
        "        '''\n",
        "\n",
        "        self.model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', output_hidden_states=True)\n",
        "        self.fc1 = torch.nn.Linear(input_dims, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 2048)\n",
        "        self.fc3 = torch.nn.Linear(2048, output_dims)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids):\n",
        "              \n",
        "        x = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        # print (x.hidden_states[0].shape)\n",
        "        x = x.hidden_states[12][0][0]\n",
        "        \n",
        "        # x = x.hidden_states[0]\n",
        "        # x = x[:,0,:]\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class VisualBERTModel:\n",
        "\n",
        "    #take necessary inputs\n",
        "    #input_dims, output_dims, batch_size_test, batch_size_train\n",
        "    def __init__(self, output_dims, lr, use_weights=False):\n",
        "        \n",
        "        self.input_dims = 768\n",
        "        self.output_dims = output_dims\n",
        "        self.model = CustomVB(self.input_dims, self.output_dims)\n",
        "        self.model = self.model.cuda()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        \n",
        "        if (use_weights):\n",
        "            self.compute_class_weights()\n",
        "        else:\n",
        "            self.loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.visual_embeddings = {}\n",
        "        self.visual_embeddings_train = {}\n",
        "        self.visual_embeddings_test = {}\n",
        "        \n",
        "        self.batch_size = 1\n",
        "        self.lr = lr\n",
        "\n",
        "    def compute_class_weights(self):\n",
        "\n",
        "        try:\n",
        "            print (f\"Total {len(ans2label)} classes, computing weights\")\n",
        "            weights = [0.0]*len(ans2label)\n",
        "            for example in train:\n",
        "                weights[int(example[2])] += 1\n",
        "\n",
        "            normedWeights = [1 - (x / sum(weights)) for x in weights]\n",
        "            weights = torch.FloatTensor(weights)\n",
        "            # /float(len(train))\n",
        "            # weights = 1.0 / weights\n",
        "            # weights = torch.nan_to_num(weights, posinf=0.0)\n",
        "            # weights = weights / weights.sum()\n",
        "            # weights = torch.nan_to_num(weights)\n",
        "            print ((weights).shape, weights)\n",
        "\n",
        "        except:\n",
        "            print (f\"{ans2label} or {train} pickle files not loaded, check environment setup\")\n",
        "\n",
        "        self.loss = torch.nn.CrossEntropyLoss(weight = weights).cuda()\n",
        "\n",
        "\n",
        "    def load_visual_embeddings(self, path, split='train'):\n",
        "\n",
        "        if split=='train':\n",
        "            self.visual_embeddings_train = pd.read_pickle(path)\n",
        "        elif split=='test':\n",
        "            self.visual_embeddings_test = pd.read_pickle(path)\n",
        "\n",
        "    def make_prediction(self, img_id, question,split='train'):\n",
        "        '''\n",
        "            should be a list of imgs/ques\n",
        "        '''\n",
        "\n",
        "        tokens = self.tokenizer(question, padding='max_length', max_length=100)\n",
        "        # tokens = self.tokenizer(question, padding='max_length', max_length=32, add \n",
        "        input_ids = torch.tensor(tokens[\"input_ids\"]).cuda() #.unsqueeze(0)\n",
        "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).cuda()\n",
        "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).cuda()\n",
        "        # visual_embeds = torch.stack(self.get_visual_embeddings(img_id)).cuda()\n",
        "\n",
        "        if split=='train':\n",
        "            for id in img_id:\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_train[img_id[0]]).cuda()        \n",
        "        else:\n",
        "            for id in img_id:\n",
        "                # print ((self.visual_embeddings_test[id]))\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_test[img_id[0]]).cuda()        \n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def test(self, batch_size=2, load_path=None, return_all=False):\n",
        "\n",
        "        num_batches = int(len(test))\n",
        "        test_loss = 0.0\n",
        "        total_correct = 0.0\n",
        "        print (\"Evaluating\")\n",
        "\n",
        "        if load_path is not None:\n",
        "            print (f'Loading path from {load_path}')\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            batch = test[i:i+self.batch_size]\n",
        "            imgs = batch[:,0]\n",
        "            questions = list(batch[:,1])\n",
        "            labels = batch[:,2]\n",
        "            if (imgs == []) or (questions == []) or (labels == []):\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'test')\n",
        "                # print (class_pred.shape)\n",
        "                y_pred = torch.log_softmax(class_pred.unsqueeze(0), dim=1)\n",
        "                _, y_pred_tags = torch.max(y_pred, dim=1)\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                if (int(labels[0]) == y_pred_tags[0].item()):\n",
        "                    total_correct+=1\n",
        "                \n",
        "                predictions.append([int(labels[0]), y_pred_tags[0].item()])\n",
        "                \n",
        "        print (f\"Test accuracy is {total_correct/len(test)} \\n\")\n",
        "\n",
        "        if (return_all == True):\n",
        "            return np.asarray(predictions)\n",
        "        else:\n",
        "            return total_correct/len(test)\n",
        "\n",
        "    def train(self, start_epoch=0, epochs=2, batch_size=1, load_path=None, save_path=None):\n",
        "\n",
        "        num_batches = int(len(train))\n",
        "        train_loss_log = []\n",
        "        test_acc_log = []\n",
        "        max_test = 0.0\n",
        "\n",
        "        print (save_path)\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        if load_path is not None:\n",
        "            print (load_path)\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        for ep in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for i in range(num_batches):\n",
        "\n",
        "                assert(self.model.training)\n",
        "\n",
        "                if (i%4000==0 and i>0):\n",
        "                    print (f'Epoch {ep}, {i}/{num_batches} batches, loss is {train_loss/i}')\n",
        "        \n",
        "                batch = train[i:i+self.batch_size]\n",
        "                imgs = batch[:,0]\n",
        "                questions = list(batch[:,1])\n",
        "                labels = batch[:,2]\n",
        "\n",
        "                if (imgs == []) or (questions == []) or (labels == []):\n",
        "                    continue\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'train')\n",
        "                # print (class_pred.shape)\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                loss = self.loss(class_pred.unsqueeze(0), gt.unsqueeze(0))\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "                if ((i+1)%batch_size==0 or i==len(train)):\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    \n",
        "            (train_loss_log.append(train_loss))\n",
        "            \n",
        "            if (save_path != None and (ep+1)%3==0):\n",
        "                torch.save({\n",
        "                    'epoch': ep,\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': train_loss_log,\n",
        "                    'vb_model_state_dict': self.model.state_dict(),\n",
        "                    'max_test': max_test,\n",
        "                    'test_acc_log': test_acc_log,\n",
        "                    'batch_size': batch_size,\n",
        "                    'lr' : self.lr\n",
        "                }, save_path+f\"/unfrozen_b{batch_size}_lr{int(self.lr)}_{ep}.pth\")\n",
        "                print (\"Saved model to: \", save_path+f\"/unfrozen_b{batch_size}_lr{int(1000000*self.lr)}_{ep}.pth\")\n",
        "\n",
        "            \n",
        "            print (f'Completed {ep+1} epochs out of {epochs}, loss is {train_loss_log[ep]/len(train)} \\n')\n",
        "            test_acc = self.test(batch_size=1)\n",
        "            try:\n",
        "                if (test_acc > max_test):\n",
        "                    max_test = test_acc\n",
        "                    torch.save({\n",
        "                        'epoch': ep,\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'loss': train_loss_log,\n",
        "                        'vb_model_state_dict': self.model.state_dict(),\n",
        "                        'max_test': max_test,\n",
        "                        'test_acc_log': test_acc_log,\n",
        "                        'batch_size': batch_size,\n",
        "                        'lr': self.lr\n",
        "                    }, save_path+f\"/unfrozen_b{batch_size}_lr{int(self.lr)}_best.pth\")\n",
        "                    print (\"Saved model to: \", save_path+f\"/unfrozen_b{batch_size}_lr{int(1000000*self.lr)}_best.pth\")\n",
        "            except:\n",
        "                print (\"Could not check for the best model\")\n",
        "            "
      ],
      "metadata": {
        "id": "fBiCEgEeZLxZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Loading model\")\n",
        "visualbert = VisualBERTModel(len(ans2label), lr=0.00001, use_weights=False)\n",
        "\n",
        "#load visual embeddings from pickle file\n",
        "print (\"Loading visual embeddings from file\")\n",
        "visual_embeddings_path = \"/content/drive/MyDrive/CS685/project/img_features\"\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_train.pkl\", 'train')\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_test.pkl\", 'test')\n",
        "\n",
        "print (\"Success\")"
      ],
      "metadata": {
        "id": "Qdy1BwrPbd8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "ac73f2de999448f0bb144aa856558f3d",
            "b81369fd32bd49a9b2b1bc38ba5a83b5",
            "78ef7943f30741cf94c29b7987d2f185",
            "dab2f740df734210b20837dccaf591b1",
            "288d91d44a88475d95f9d97f8ca23b3b",
            "a940648bf97e4b4ab64d0217b752786f",
            "12ba5b06606e48fdbad651a237cdc283",
            "43f153c2b74c4dd0b92776c57d0a2aab",
            "407b2509dea140af891dcece39a7716d",
            "5222d1f5cb914444855602faeb7c82a0",
            "43be34c52b3f46449322ddd0a4ea1c82",
            "90a1fce82d6e425da5e2b4650f4287a5",
            "d6908fc964134aaaa2ee0f6aa283519d",
            "1f5f6cd02dc84ef4a272bcaa40189fc5",
            "e42980c66ce849c3b0ded48c2ebba5d9",
            "7181d7c731cb4b71924e43fe1135aa15",
            "5cf299a9e49747208a92b2f41990916d",
            "5672465f535d47d3ab15888c5ff040b5",
            "98c211afa4b24033912613457f13aa46",
            "5e51d3c50f194c059f849d689d79b294",
            "31bacdc6b8824c2eb96c721a5201caa5",
            "7a363a31e630467aa9108906d6005349",
            "213482ab902641399307d1596beccd49",
            "b2c1d4da92d84130839d63d70addf0e2",
            "d72dba0ce3ea43fe841b95e4bc335e16",
            "524365c5d452400faea4e4dfef8985e1",
            "2e165baa6c654649b560a527afa0b258",
            "f32bc41eb4be49d9b5576d983119ab08",
            "8b8aa1e8e0bd47b090523a94d64d5f3d",
            "cbec7e61e9ea46c6878450f81e575be3",
            "7b21609097434fcd9b12fd5d1ab65da9",
            "f0edc096fc9a49098a4e716738c8d739",
            "96d0d808e5224bdbbdeef0be5c1d65f4",
            "7f60a15c1a2a4d48bfa942c9c16ea85f",
            "385896af6d864adbb4c901d96cedf06a",
            "cce1aeb99e2a4648a8828f1211b9302e",
            "ad58c81aedc44b1aa0e1d5fe0895507b",
            "3e0c87cd3ff841e1bed0a35aaf1e0ea2",
            "59e601e114414a5ca827b8118d76e7ff",
            "10c29a5df94a4549a40392015dc8228e",
            "348431e779ea42a09a4062d74940f460",
            "1147fde3aaf049e08ecd402f19ac151d",
            "f362fb3b948d41ed830f837529f25a23",
            "7cf7015105e343d7b98046b5ae0ee4f1",
            "4ad45aa2c54a408da7b3ecc7513774d9",
            "3824419b611047bdb935866f4f2bd29c",
            "de81c8dd2c8345f9bc42c40539830eb1",
            "231b62568aef49458d807aef909b7332",
            "83235179f394421cae09ee2355795a26",
            "4621427fc443434ba1a175aa1e86690e",
            "1c07c47407784238a7e94ba71ab4de2c",
            "8af3f61b20104c06850e55c836280ef1",
            "772edd7d7d77428e8211990f9b6f3503",
            "cc759dda77c1437c9d2982b7e33b35c1",
            "d5bec50bde594e3fbeb8888c4a326ced",
            "275874e139d943e88c597e79c39821d1",
            "ba09a5f9b0e141a6a522cfe0eacff617",
            "24f02218f23b4a29a722b14d2dea623e",
            "82c5d61460eb42028bc076dabb271045",
            "d9e41b33ae184f6592d37249a1606111",
            "a1e0f09b2f624b0aa6d41d804ff33073",
            "44378193b7db4dcba3324d82bd291b97",
            "290e05050ec64fa88b1b96bc7631d6c8",
            "92332fc5a4a647f988151033610397ae",
            "3df474e0067249d789f2190e39d4c623",
            "3f73b05dbeef46a98cd9c9e20ed8e849"
          ]
        },
        "outputId": "92fb86a6-c08d-4a76-b9de-e10d592f67bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac73f2de999448f0bb144aa856558f3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90a1fce82d6e425da5e2b4650f4287a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/425M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "213482ab902641399307d1596beccd49",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f60a15c1a2a4d48bfa942c9c16ea85f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ad45aa2c54a408da7b3ecc7513774d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "275874e139d943e88c597e79c39821d1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading visual embeddings from file\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = None #'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\n",
        "visualbert.loss = visualbert.loss.cuda()\n",
        "visualbert.train(epochs=20,batch_size=8,save_path='/content/drive/MyDrive/CS685/project/vb/hs',load_path=load_path)"
      ],
      "metadata": {
        "id": "gRxGf5gfbmL0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19d21e82-ae44-4a65-95f5-37996963b56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/project/vb/hs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:202: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, 4000/19755 batches, loss is 5.432911711528897\n",
            "Epoch 0, 8000/19755 batches, loss is 4.607990379914641\n",
            "Epoch 0, 12000/19755 batches, loss is 4.394262281943423\n",
            "Epoch 0, 16000/19755 batches, loss is 4.2308385299672375\n",
            "Completed 1 epochs out of 20, loss is 4.091028908154803 \n",
            "\n",
            "Evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:141: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 0.3208132322536182 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 1, 4000/19755 batches, loss is 3.9027823251811786\n",
            "Epoch 1, 8000/19755 batches, loss is 3.0773398000537417\n",
            "Epoch 1, 12000/19755 batches, loss is 2.827122273199532\n",
            "Epoch 1, 16000/19755 batches, loss is 2.707432395949989\n",
            "Completed 2 epochs out of 20, loss is 2.582336950076235 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4405582356995176 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 2, 4000/19755 batches, loss is 3.4968660045759754\n",
            "Epoch 2, 8000/19755 batches, loss is 2.746453888438846\n",
            "Epoch 2, 12000/19755 batches, loss is 2.5389018848154423\n",
            "Epoch 2, 16000/19755 batches, loss is 2.412454244262095\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_2.pth\n",
            "Completed 3 epochs out of 20, loss is 2.2906906412851225 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4703652653342522 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 3, 4000/19755 batches, loss is 3.3099994515022844\n",
            "Epoch 3, 8000/19755 batches, loss is 2.6134617225298715\n",
            "Epoch 3, 12000/19755 batches, loss is 2.413586600249284\n",
            "Epoch 3, 16000/19755 batches, loss is 2.2826280268143675\n",
            "Completed 4 epochs out of 20, loss is 2.1625261749196962 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.45813232253618197 \n",
            "\n",
            "Epoch 4, 4000/19755 batches, loss is 3.1460854554749096\n",
            "Epoch 4, 8000/19755 batches, loss is 2.512678113039292\n",
            "Epoch 4, 12000/19755 batches, loss is 2.3286729992969195\n",
            "Epoch 4, 16000/19755 batches, loss is 2.1949129037805033\n",
            "Completed 5 epochs out of 20, loss is 2.0737429262145652 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4784631288766368 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 5, 4000/19755 batches, loss is 3.045229085359955\n",
            "Epoch 5, 8000/19755 batches, loss is 2.4400111877335005\n",
            "Epoch 5, 12000/19755 batches, loss is 2.263081105360548\n",
            "Epoch 5, 16000/19755 batches, loss is 2.129648678319234\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_5.pth\n",
            "Completed 6 epochs out of 20, loss is 2.012810383062671 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.46950379048931773 \n",
            "\n",
            "Epoch 6, 4000/19755 batches, loss is 2.951775600977475\n",
            "Epoch 6, 8000/19755 batches, loss is 2.3702070826774286\n",
            "Epoch 6, 12000/19755 batches, loss is 2.203115883140747\n",
            "Epoch 6, 16000/19755 batches, loss is 2.084201498674036\n",
            "Completed 7 epochs out of 20, loss is 2.010119016870993 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.465368711233632 \n",
            "\n",
            "Epoch 7, 4000/19755 batches, loss is 2.904255636438611\n",
            "Epoch 7, 8000/19755 batches, loss is 2.383803370851223\n",
            "Epoch 7, 12000/19755 batches, loss is 2.213354554041803\n",
            "Epoch 7, 16000/19755 batches, loss is 2.072934649251052\n",
            "Completed 8 epochs out of 20, loss is 1.9573471548936272 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4848380427291523 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 8, 4000/19755 batches, loss is 2.825373613076401\n",
            "Epoch 8, 8000/19755 batches, loss is 2.3050343680843945\n",
            "Epoch 8, 12000/19755 batches, loss is 2.140549815366074\n",
            "Epoch 8, 16000/19755 batches, loss is 2.0052295263020676\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_8.pth\n",
            "Completed 9 epochs out of 20, loss is 1.8977148805358397 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4879393521709166 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_best.pth\n",
            "Epoch 9, 4000/19755 batches, loss is 2.780240051046654\n",
            "Epoch 9, 8000/19755 batches, loss is 2.279008201481498\n",
            "Epoch 9, 12000/19755 batches, loss is 2.1192782833933985\n",
            "Epoch 9, 16000/19755 batches, loss is 1.984436284165\n",
            "Completed 10 epochs out of 20, loss is 1.8711655760309687 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4564093728463129 \n",
            "\n",
            "Epoch 10, 4000/19755 batches, loss is 2.710913966703607\n",
            "Epoch 10, 8000/19755 batches, loss is 2.2187477364084627\n",
            "Epoch 10, 12000/19755 batches, loss is 2.06980507290894\n",
            "Epoch 10, 16000/19755 batches, loss is 1.9508410326606935\n",
            "Completed 11 epochs out of 20, loss is 1.8486402327826623 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.46433494141971055 \n",
            "\n",
            "Epoch 11, 4000/19755 batches, loss is 2.6467058405766\n",
            "Epoch 11, 8000/19755 batches, loss is 2.1881358682067296\n",
            "Epoch 11, 12000/19755 batches, loss is 2.045499601511008\n",
            "Epoch 11, 16000/19755 batches, loss is 1.9262035023812314\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/hs/unfrozen_b8_lr10_11.pth\n",
            "Completed 12 epochs out of 20, loss is 1.823971062148831 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.45554789800137835 \n",
            "\n",
            "Epoch 12, 4000/19755 batches, loss is 2.6020689458183917\n",
            "Epoch 12, 8000/19755 batches, loss is 2.1617042864468092\n",
            "Epoch 12, 12000/19755 batches, loss is 2.022360542690677\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b7b527a2ea84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m#'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CS685/project/vb/hs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-48ebbc88f3f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_epoch, epochs, batch_size, load_path, save_path)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mclass_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;31m# print (class_pred.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-48ebbc88f3f0>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(self, img_id, question, split)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mvisual_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mvisual_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_token_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-48ebbc88f3f0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_token_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual_token_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisual_token_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# print (x.hidden_states[0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict, labels, sentence_image_labels)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m         )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m             )\n\u001b[1;32m    852\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 )\n\u001b[1;32m    435\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/visual_bert/modeling_visual_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         )\n\u001b[1;32m    386\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m     \u001b[0;31m# inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_args_in_forward_chunk_fn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2831\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0;32m-> 2833\u001b[0;31m                                         follow_wrapper_chains=follow_wrapped)\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2215\u001b[0m             \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m             \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m             sigcls=sigcls)\n\u001b[0m\u001b[1;32m   2218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_signature_from_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_signature_is_builtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m     \u001b[0;31m# ... w/ defaults.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_default_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         parameters.append(Parameter(name, annotation=annotation,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test accuracy for the given model\n",
        "load_path = '/content/drive/MyDrive/CS685/project/vb/unfrozen/unfrozen_b8_lr0_best.pth'\n",
        "preds = visualbert.test(batch_size=1,load_path=load_path, return_all=True)"
      ],
      "metadata": {
        "id": "bfqk9PhI1dp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "IaG3PassXyht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline('fill-mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "ukU4RsAjaQli",
        "outputId": "61079125-831e-4029-c565-143ff30d9d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7041437074a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fill-mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0rc1\n",
        "import pickle\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "\n",
        "languages = [\n",
        "    'en', # english\n",
        "    'cs',  # czech\n",
        "    'de',  # german\n",
        "    'es', # spanish\n",
        "    'fi',  # finnish\n",
        "    'fr', # french\n",
        "    'hi', # hindi\n",
        "    'it', # italian\n",
        "    'ja', # japanese\n",
        "    'pt', # portuguese\n",
        "    'ru', # russian\n",
        "    'vi', # vietnamese\n",
        "    'zh-cn',  # chinese\n",
        "    ]\n",
        "num_langs = len(languages)\n",
        "\n",
        "def paraphrase_q(ques):\n",
        "\n",
        "    ques = ques.split(' ')\n",
        "    n = len(ques)\n",
        "    idx = np.random.randint(0,n,1)\n",
        "    ques[int(idx)] = '<mask>'\n",
        "    ques = ' '.join(ques)\n",
        "    paraphrased = nlp(ques)[0]\n",
        "    return paraphrased['sequence']\n",
        "\n",
        "def backtranslate(ques):\n",
        "\n",
        "    target_idx = np.random.randint(low=1, high=num_langs, size=1)\n",
        "    translated_example = translator.translate(ques, src='en', languages[idx])\n",
        "    backtranslated_ques = translator.translate(translated_example.text, src=languages[idx], src='en')\n",
        "    return backtranslated_ques.text\n",
        "\n",
        "def augment_data(train, ratio=0.2, augment_type='replace_mask'):\n",
        "\n",
        "    augmented_train = list(train)\n",
        "    augmentation_examples = {}\n",
        "\n",
        "    path = '/content/drive/MyDrive/PathVQA/split/qas/'\n",
        "    num_examples_to_augment = int(ratio*len(train))\n",
        "    examples_to_augment = np.random.randint(0, len(train), num_examples_to_augment)\n",
        "\n",
        "    cnt = 0    \n",
        "    for idx in examples_to_augment:\n",
        "        img_id, ques, label = train[idx]\n",
        "        if (augment_type='replace_mask'):\n",
        "            q = paraphrase_q(ques)\n",
        "        else:\n",
        "            q = backtranslate(ques)\n",
        "            \n",
        "        if (q!=ques):\n",
        "            print (ques, \"\\n\", q, \"\\n\")\n",
        "            augmentation_examples[cnt] = [ques, q]\n",
        "            cnt += 1\n",
        "            augmented_train.append([img_id, q, label])\n",
        "\n",
        "\n",
        "    save_path = \"/content/drive/MyDrive/CS685/project/augmented\"\n",
        "    f = open(save_path + f\"{augment_type}_examples.pkl\",\"wb\")\n",
        "    pickle.dump(augmentation_examples,f)\n",
        "    f.close()\n",
        "\n",
        "    f = open(save_path + f\"{augment_type}_train.pkl\",\"wb\")\n",
        "    pickle.dump(augmented_train,f)\n",
        "    f.close()\n",
        "    \n",
        "    print (f'Length of dataset before augmentation : {len(train)}')\n",
        "    print (f'Length of dataset after augmentation : {len(augmented_train)}')\n",
        "\n",
        "    return augmented_train\n",
        "\n",
        "augmented_train= augment_data(train, 0.15)"
      ],
      "metadata": {
        "id": "PdsbF-HKoYCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_augmented_data():\n",
        "\n",
        "    path = \"/content/drive/MyDrive/CS685/project/augmented_train.pkl\"    \n",
        "    train_qa = pd.read_pickle(path)    \n",
        "    return np.asarray(train_qa)\n",
        "\n",
        "train = load_augmented_data()"
      ],
      "metadata": {
        "id": "X-3ZlMwcx4YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertForPreTraining\n",
        "\n",
        "class CustomVB(torch.nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "            Integrates the classification head on top ob base VisualBERT\n",
        "            Loss is backpropogated throughout the model\n",
        "        '''\n",
        "\n",
        "        self.model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', output_hidden_states=True)\n",
        "        self.fc1 = torch.nn.Linear(input_dims, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 2048)\n",
        "        self.fc3 = torch.nn.Linear(2048, output_dims)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids):\n",
        "              \n",
        "        x = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        x = x.hidden_states[11][0][0]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class VisualBERTModel:\n",
        "\n",
        "    #take necessary inputs\n",
        "    #input_dims, output_dims, batch_size_test, batch_size_train\n",
        "    def __init__(self, output_dims, lr, use_weights=False):\n",
        "        \n",
        "        self.input_dims = 768\n",
        "        self.output_dims = output_dims\n",
        "        self.model = CustomVB(self.input_dims, self.output_dims)\n",
        "        self.model = self.model.cuda()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        \n",
        "        if (use_weights):\n",
        "            self.compute_class_weights()\n",
        "        else:\n",
        "            self.loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.visual_embeddings = {}\n",
        "        self.visual_embeddings_train = {}\n",
        "        self.visual_embeddings_test = {}\n",
        "        \n",
        "        self.batch_size = 1\n",
        "        self.lr = lr\n",
        "\n",
        "    def compute_class_weights(self):\n",
        "\n",
        "        try:\n",
        "            print (f\"Total {len(ans2label)} classes, computing weights\")\n",
        "            weights = [0.0]*len(ans2label)\n",
        "            for example in train:\n",
        "                weights[int(example[2])] += 1\n",
        "\n",
        "            normedWeights = [1 - (x / sum(weights)) for x in weights]\n",
        "            weights = torch.FloatTensor(weights)\n",
        "            # /float(len(train))\n",
        "            # weights = 1.0 / weights\n",
        "            # weights = torch.nan_to_num(weights, posinf=0.0)\n",
        "            # weights = weights / weights.sum()\n",
        "            # weights = torch.nan_to_num(weights)\n",
        "            print ((weights).shape, weights)\n",
        "\n",
        "        except:\n",
        "            print (f\"{ans2label} or {train} pickle files not loaded, check environment setup\")\n",
        "\n",
        "        self.loss = torch.nn.CrossEntropyLoss(weight = weights).cuda()\n",
        "\n",
        "\n",
        "    def load_visual_embeddings(self, path, split='train'):\n",
        "\n",
        "        if split=='train':\n",
        "            self.visual_embeddings_train = pd.read_pickle(path)\n",
        "        elif split=='test':\n",
        "            self.visual_embeddings_test = pd.read_pickle(path)\n",
        "\n",
        "    def make_prediction(self, img_id, question,split='train'):\n",
        "        '''\n",
        "            should be a list of imgs/ques\n",
        "        '''\n",
        "\n",
        "        tokens = self.tokenizer(question, padding='max_length', max_length=100)\n",
        "        input_ids = torch.tensor(tokens[\"input_ids\"]).cuda() #.unsqueeze(0)\n",
        "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).cuda()\n",
        "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).cuda()\n",
        "        # visual_embeds = torch.stack(self.get_visual_embeddings(img_id)).cuda()\n",
        "\n",
        "        if split=='train':\n",
        "            for id in img_id:\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_train[img_id[0]]).cuda()        \n",
        "        else:\n",
        "            for id in img_id:\n",
        "                # print ((self.visual_embeddings_test[id]))\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_test[img_id[0]]).cuda()        \n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def test(self, batch_size=2, load_path=None, return_all=False):\n",
        "\n",
        "        num_batches = int(len(test))\n",
        "        test_loss = 0.0\n",
        "        total_correct = 0.0\n",
        "        print (\"Evaluating\")\n",
        "\n",
        "        if load_path is not None:\n",
        "            print (f'Loading path from {load_path}')\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            batch = test[i:i+self.batch_size]\n",
        "            imgs = batch[:,0]\n",
        "            questions = list(batch[:,1])\n",
        "            labels = batch[:,2]\n",
        "            if (imgs == []) or (questions == []) or (labels == []):\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'test')\n",
        "                y_pred = torch.log_softmax(class_pred.unsqueeze(0), dim=1)\n",
        "                _, y_pred_tags = torch.max(y_pred, dim=1)\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                if (int(labels[0]) == y_pred_tags[0].item()):\n",
        "                    total_correct+=1\n",
        "                \n",
        "                predictions.append([int(labels[0]), y_pred_tags[0].item()])\n",
        "                \n",
        "        print (f\"Test accuracy is {total_correct/len(test)} \\n\")\n",
        "\n",
        "        if (return_all == True):\n",
        "            return predictions\n",
        "        else:\n",
        "            return total_correct/len(test)\n",
        "\n",
        "    def train(self, start_epoch=0, epochs=2, batch_size=1, load_path=None, save_path=None):\n",
        "\n",
        "        num_batches = int(len(train))\n",
        "        train_loss_log = []\n",
        "        test_acc_log = []\n",
        "        max_test = 0.0\n",
        "\n",
        "        print (save_path)\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        if load_path is not None:\n",
        "            print (load_path)\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        for ep in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for i in range(num_batches):\n",
        "\n",
        "                assert(self.model.training)\n",
        "\n",
        "                if (i%4000==0 and i>0):\n",
        "                    print (f'Epoch {ep}, {i}/{num_batches} batches, loss is {train_loss/i}')\n",
        "        \n",
        "                batch = (train[i:i+self.batch_size])\n",
        "                imgs = batch[:,0]\n",
        "                questions = list(batch[:,1])\n",
        "                labels = batch[:,2]\n",
        "\n",
        "                if (imgs == []) or (questions == []) or (labels == []):\n",
        "                    continue\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'train')\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                loss = self.loss(class_pred.unsqueeze(0), gt.unsqueeze(0))\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "                if ((i+1)%batch_size==0 or i==len(train)):\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    \n",
        "            (train_loss_log.append(train_loss))\n",
        "            \n",
        "            if (save_path != None and (ep+1)%3==0):\n",
        "                torch.save({\n",
        "                    'epoch': ep,\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': train_loss_log,\n",
        "                    'vb_model_state_dict': self.model.state_dict(),\n",
        "                    'max_test': max_test,\n",
        "                    'test_acc_log': test_acc_log,\n",
        "                    'batch_size': batch_size,\n",
        "                    'lr' : self.lr\n",
        "                }, save_path+f\"/augmented_b{batch_size}_lr{int(self.lr)}_{ep}.pth\")\n",
        "                print (\"Saved model to: \", save_path+f\"/augmented_b{batch_size}_lr{int(1000000*self.lr)}_{ep}.pth\")\n",
        "\n",
        "            \n",
        "            print (f'Completed {ep+1} epochs out of {epochs}, loss is {train_loss_log[ep]/len(train)} \\n')\n",
        "            test_acc = self.test(batch_size=1)\n",
        "            try:\n",
        "                if (test_acc > max_test):\n",
        "                    max_test = test_acc\n",
        "                    torch.save({\n",
        "                        'epoch': ep,\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'loss': train_loss_log,\n",
        "                        'vb_model_state_dict': self.model.state_dict(),\n",
        "                        'max_test': max_test,\n",
        "                        'test_acc_log': test_acc_log,\n",
        "                        'batch_size': batch_size,\n",
        "                        'lr': self.lr\n",
        "                    }, save_path+f\"/augmented_b{batch_size}_lr{int(self.lr)}_best.pth\")\n",
        "                    print (\"Saved model to: \", save_path+f\"/augmented_b{batch_size}_lr{int(1000000*self.lr)}_best.pth\")\n",
        "            except:\n",
        "                print (\"Could not check for the best model\")"
      ],
      "metadata": {
        "id": "sIzAv7r-wFlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Loading model\")\n",
        "visualbert = VisualBERTModel(len(ans2label), lr=0.00001, use_weights=False)\n",
        "\n",
        "#load visual embeddings from pickle file\n",
        "print (\"Loading visual embeddings from file\")\n",
        "visual_embeddings_path = \"/content/drive/MyDrive/CS685/project/img_features\"\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_train.pkl\", 'train')\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_test.pkl\", 'test')\n",
        "\n",
        "print (\"Success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktbM5xqcwFrE",
        "outputId": "1b2d42f7-ca67-41dd-c3a0-809dd4e002b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model\n",
            "Loading visual embeddings from file\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = augmented_train\n",
        "# print (train[0])\n",
        "load_path = None #'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\n",
        "visualbert.loss = visualbert.loss.cuda()\n",
        "visualbert.train(epochs=20,batch_size=8,save_path='/content/drive/MyDrive/CS685/project/vb/augmented',load_path=load_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCYKJXo4wXkD",
        "outputId": "a44b1de4-b413-403c-8703-70d3e63788ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/project/vb/augmented\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:194: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, 4000/22051 batches, loss is 5.2291851657256485\n",
            "Epoch 0, 8000/22051 batches, loss is 4.379922498886473\n",
            "Epoch 0, 12000/22051 batches, loss is 4.11580168449316\n",
            "Epoch 0, 16000/22051 batches, loss is 3.927877879515894\n",
            "Epoch 0, 20000/22051 batches, loss is 3.7756617886486215\n",
            "Completed 1 epochs out of 20, loss is 3.7224465140403518 \n",
            "\n",
            "Evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:134: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 0.48862853204686424 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 1, 4000/22051 batches, loss is 3.9510797196064957\n",
            "Epoch 1, 8000/22051 batches, loss is 2.986276704346048\n",
            "Epoch 1, 12000/22051 batches, loss is 2.7146937706920724\n",
            "Epoch 1, 16000/22051 batches, loss is 2.602974610980036\n",
            "Epoch 1, 20000/22051 batches, loss is 2.493798980933771\n",
            "Completed 2 epochs out of 20, loss is 2.523988846100198 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5075809786354238 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 2, 4000/22051 batches, loss is 3.48714043974239\n",
            "Epoch 2, 8000/22051 batches, loss is 2.6173380722278963\n",
            "Epoch 2, 12000/22051 batches, loss is 2.3670299600637277\n",
            "Epoch 2, 16000/22051 batches, loss is 2.2581948030234926\n",
            "Epoch 2, 20000/22051 batches, loss is 2.154470825381561\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_2.pth\n",
            "Completed 3 epochs out of 20, loss is 2.1891548023261764 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5120606478290833 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 3, 4000/22051 batches, loss is 3.2146510093893013\n",
            "Epoch 3, 8000/22051 batches, loss is 2.404638776814019\n",
            "Epoch 3, 12000/22051 batches, loss is 2.1713218269833865\n",
            "Epoch 3, 16000/22051 batches, loss is 2.0621429841238914\n",
            "Epoch 3, 20000/22051 batches, loss is 1.957563802210863\n",
            "Completed 4 epochs out of 20, loss is 1.9947880009737085 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5151619572708477 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 4, 4000/22051 batches, loss is 3.0179075513624665\n",
            "Epoch 4, 8000/22051 batches, loss is 2.257912081053757\n",
            "Epoch 4, 12000/22051 batches, loss is 2.0375574225531308\n",
            "Epoch 4, 16000/22051 batches, loss is 1.9320968547626598\n",
            "Epoch 4, 20000/22051 batches, loss is 1.8274207338382693\n",
            "Completed 5 epochs out of 20, loss is 1.865288287892257 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5223983459682977 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 5, 4000/22051 batches, loss is 2.8377367905794237\n",
            "Epoch 5, 8000/22051 batches, loss is 2.1268618105726884\n",
            "Epoch 5, 12000/22051 batches, loss is 1.9270719792762279\n",
            "Epoch 5, 16000/22051 batches, loss is 1.8283005265482049\n",
            "Epoch 5, 20000/22051 batches, loss is 1.7248323531553706\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_5.pth\n",
            "Completed 6 epochs out of 20, loss is 1.7642285015019714 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5232598208132323 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 6, 4000/22051 batches, loss is 2.7092106741103454\n",
            "Epoch 6, 8000/22051 batches, loss is 2.0276857542751068\n",
            "Epoch 6, 12000/22051 batches, loss is 1.8409808169849367\n",
            "Epoch 6, 16000/22051 batches, loss is 1.7483045239871102\n",
            "Epoch 6, 20000/22051 batches, loss is 1.6452814521755896\n",
            "Completed 7 epochs out of 20, loss is 1.6851382035292102 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.538249483115093 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 7, 4000/22051 batches, loss is 2.6009763168554074\n",
            "Epoch 7, 8000/22051 batches, loss is 1.9474744153838788\n",
            "Epoch 7, 12000/22051 batches, loss is 1.7708846224077959\n",
            "Epoch 7, 16000/22051 batches, loss is 1.682904806753284\n",
            "Epoch 7, 20000/22051 batches, loss is 1.583036794781969\n",
            "Completed 8 epochs out of 20, loss is 1.6208932678007646 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5106822880771882 \n",
            "\n",
            "Epoch 8, 4000/22051 batches, loss is 2.4980461258751165\n",
            "Epoch 8, 8000/22051 batches, loss is 1.8717626988880693\n",
            "Epoch 8, 12000/22051 batches, loss is 1.7054135296033115\n",
            "Epoch 8, 16000/22051 batches, loss is 1.6235182709199234\n",
            "Epoch 8, 20000/22051 batches, loss is 1.5262697265596028\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_8.pth\n",
            "Completed 9 epochs out of 20, loss is 1.5637995772326083 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5365265334252239 \n",
            "\n",
            "Epoch 9, 4000/22051 batches, loss is 2.4047243589118703\n",
            "Epoch 9, 8000/22051 batches, loss is 1.8056910160195463\n",
            "Epoch 9, 12000/22051 batches, loss is 1.649416376931567\n",
            "Epoch 9, 16000/22051 batches, loss is 1.5703698178213212\n",
            "Epoch 9, 20000/22051 batches, loss is 1.4744870729261041\n",
            "Completed 10 epochs out of 20, loss is 1.5104632051758708 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5430737422467264 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_best.pth\n",
            "Epoch 10, 4000/22051 batches, loss is 2.338022812803618\n",
            "Epoch 10, 8000/22051 batches, loss is 1.759351901171619\n",
            "Epoch 10, 12000/22051 batches, loss is 1.6098950925715334\n",
            "Epoch 10, 16000/22051 batches, loss is 1.5323973649384135\n",
            "Epoch 10, 20000/22051 batches, loss is 1.4348568245312419\n",
            "Completed 11 epochs out of 20, loss is 1.46841479487229 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5208476912474156 \n",
            "\n",
            "Epoch 11, 4000/22051 batches, loss is 2.2616183692640646\n",
            "Epoch 11, 8000/22051 batches, loss is 1.711235562973624\n",
            "Epoch 11, 12000/22051 batches, loss is 1.5686809650826723\n",
            "Epoch 11, 16000/22051 batches, loss is 1.4971753169026896\n",
            "Epoch 11, 20000/22051 batches, loss is 1.4049507365717362\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_11.pth\n",
            "Completed 12 epochs out of 20, loss is 1.43485911589973 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5236044107512061 \n",
            "\n",
            "Epoch 12, 4000/22051 batches, loss is 2.164321012701406\n",
            "Epoch 12, 8000/22051 batches, loss is 1.6533887303933783\n",
            "Epoch 12, 12000/22051 batches, loss is 1.5225141267783404\n",
            "Epoch 12, 16000/22051 batches, loss is 1.4530824569671683\n",
            "Epoch 12, 20000/22051 batches, loss is 1.3629320251074801\n",
            "Completed 13 epochs out of 20, loss is 1.3917395354177717 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5299793246037215 \n",
            "\n",
            "Epoch 13, 4000/22051 batches, loss is 2.0841418381712713\n",
            "Epoch 13, 8000/22051 batches, loss is 1.5945691386358003\n",
            "Epoch 13, 12000/22051 batches, loss is 1.4823502659430399\n",
            "Epoch 13, 16000/22051 batches, loss is 1.4156095701686018\n",
            "Epoch 13, 20000/22051 batches, loss is 1.3291510036242697\n",
            "Completed 14 epochs out of 20, loss is 1.3554746850215262 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5253273604410751 \n",
            "\n",
            "Epoch 14, 4000/22051 batches, loss is 2.036424454251567\n",
            "Epoch 14, 8000/22051 batches, loss is 1.5622163400731524\n",
            "Epoch 14, 12000/22051 batches, loss is 1.450843308962095\n",
            "Epoch 14, 16000/22051 batches, loss is 1.380729958681179\n",
            "Epoch 14, 20000/22051 batches, loss is 1.29521258290475\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_14.pth\n",
            "Completed 15 epochs out of 20, loss is 1.3206107390387036 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5325637491385251 \n",
            "\n",
            "Epoch 15, 4000/22051 batches, loss is 1.9678326286470096\n",
            "Epoch 15, 8000/22051 batches, loss is 1.5235784462999138\n",
            "Epoch 15, 12000/22051 batches, loss is 1.4186140927667938\n",
            "Epoch 15, 16000/22051 batches, loss is 1.3534868263921649\n",
            "Epoch 15, 20000/22051 batches, loss is 1.2674151815532262\n",
            "Completed 16 epochs out of 20, loss is 1.2895116098298978 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5184355616815989 \n",
            "\n",
            "Epoch 16, 4000/22051 batches, loss is 1.8916319977271823\n",
            "Epoch 16, 8000/22051 batches, loss is 1.4720691257331886\n",
            "Epoch 16, 12000/22051 batches, loss is 1.3756687344733884\n",
            "Epoch 16, 16000/22051 batches, loss is 1.3123455072094554\n",
            "Epoch 16, 20000/22051 batches, loss is 1.2297389090941298\n",
            "Completed 17 epochs out of 20, loss is 1.2512872540748001 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5270503101309442 \n",
            "\n",
            "Epoch 17, 4000/22051 batches, loss is 1.812023012089784\n",
            "Epoch 17, 8000/22051 batches, loss is 1.430735517744892\n",
            "Epoch 17, 12000/22051 batches, loss is 1.3436763081784155\n",
            "Epoch 17, 16000/22051 batches, loss is 1.282117886923472\n",
            "Epoch 17, 20000/22051 batches, loss is 1.2030845547886344\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr10_17.pth\n",
            "Completed 18 epochs out of 20, loss is 1.225783896337618 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.541867677463818 \n",
            "\n",
            "Epoch 18, 4000/22051 batches, loss is 1.772619678727852\n",
            "Epoch 18, 8000/22051 batches, loss is 1.4020892772381315\n",
            "Epoch 18, 12000/22051 batches, loss is 1.3124038813725334\n",
            "Epoch 18, 16000/22051 batches, loss is 1.2562552786458938\n",
            "Epoch 18, 20000/22051 batches, loss is 1.1782021767886315\n",
            "Completed 19 epochs out of 20, loss is 1.2022889242929633 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.5263611302549965 \n",
            "\n",
            "Epoch 19, 4000/22051 batches, loss is 1.6981624574493497\n",
            "Epoch 19, 8000/22051 batches, loss is 1.3529537306937585\n",
            "Epoch 19, 12000/22051 batches, loss is 1.28517589670193\n",
            "Epoch 19, 16000/22051 batches, loss is 1.2355926504049295\n",
            "Epoch 19, 20000/22051 batches, loss is 1.15690055662126\n",
            "Completed 20 epochs out of 20, loss is 1.1787280653263241 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.518263266712612 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does VisualBERT really see?"
      ],
      "metadata": {
        "id": "8diSc0rCOYwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from transformers import BertTokenizer, VisualBertForQuestionAnswering, VisualBertForPreTraining\n",
        "\n",
        "class CustomVB(torch.nn.Module):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__()\n",
        "\n",
        "        '''\n",
        "            Integrates the classification head on top ob base VisualBERT\n",
        "            Loss is backpropogated throughout the model\n",
        "        '''\n",
        "\n",
        "        self.model = VisualBertForPreTraining.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', output_hidden_states=True)\n",
        "        self.fc1 = torch.nn.Linear(input_dims, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 2048)\n",
        "        self.fc3 = torch.nn.Linear(2048, output_dims)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids):\n",
        "              \n",
        "        x = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        x = x.hidden_states[11][0][0]\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class VisualBERTModel:\n",
        "\n",
        "    #take necessary inputs\n",
        "    #input_dims, output_dims, batch_size_test, batch_size_train\n",
        "    def __init__(self, output_dims, lr, use_weights=False):\n",
        "        \n",
        "        self.input_dims = 768\n",
        "        self.output_dims = output_dims\n",
        "        self.model = CustomVB(self.input_dims, self.output_dims)\n",
        "        self.model = self.model.cuda()\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        \n",
        "        if (use_weights):\n",
        "            self.compute_class_weights()\n",
        "        else:\n",
        "            self.loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.visual_embeddings = {}\n",
        "        self.visual_embeddings_train = {}\n",
        "        self.visual_embeddings_test = {}\n",
        "        \n",
        "        self.batch_size = 1\n",
        "        self.lr = lr\n",
        "\n",
        "    def compute_class_weights(self):\n",
        "\n",
        "        try:\n",
        "            print (f\"Total {len(ans2label)} classes, computing weights\")\n",
        "            weights = [0.0]*len(ans2label)\n",
        "            for example in train:\n",
        "                weights[int(example[2])] += 1\n",
        "\n",
        "            normedWeights = [1 - (x / sum(weights)) for x in weights]\n",
        "            weights = torch.FloatTensor(weights)\n",
        "            # /float(len(train))\n",
        "            # weights = 1.0 / weights\n",
        "            # weights = torch.nan_to_num(weights, posinf=0.0)\n",
        "            # weights = weights / weights.sum()\n",
        "            # weights = torch.nan_to_num(weights)\n",
        "            print ((weights).shape, weights)\n",
        "\n",
        "        except:\n",
        "            print (f\"{ans2label} or {train} pickle files not loaded, check environment setup\")\n",
        "\n",
        "        self.loss = torch.nn.CrossEntropyLoss(weight = weights).cuda()\n",
        "\n",
        "\n",
        "    def load_visual_embeddings(self, path, split='train'):\n",
        "\n",
        "        if split=='train':\n",
        "            self.visual_embeddings_train = pd.read_pickle(path)\n",
        "        elif split=='test':\n",
        "            self.visual_embeddings_test = pd.read_pickle(path)\n",
        "\n",
        "    def make_prediction(self, img_id, question,split='train'):\n",
        "        '''\n",
        "            should be a list of imgs/ques\n",
        "        '''\n",
        "\n",
        "        tokens = self.tokenizer(question, padding='max_length', max_length=100)\n",
        "        input_ids = torch.tensor(tokens[\"input_ids\"]).cuda() #.unsqueeze(0)\n",
        "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).cuda()\n",
        "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).cuda()\n",
        "        # visual_embeds = torch.stack(self.get_visual_embeddings(img_id)).cuda()\n",
        "\n",
        "        if split=='train':\n",
        "            for id in img_id:\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_train[img_id[0]]).cuda()        \n",
        "        else:\n",
        "            for id in img_id:\n",
        "                # print ((self.visual_embeddings_test[id]))\n",
        "                visual_embeds = torch.stack(self.visual_embeddings_test[img_id[0]]).cuda()        \n",
        "\n",
        "        visual_embed = torch.zeros(visual_embeds.shape).cuda()\n",
        "\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long).cuda()\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def test(self, batch_size=2, load_path=None, return_all=False):\n",
        "\n",
        "        num_batches = int(len(test))\n",
        "        test_loss = 0.0\n",
        "        total_correct = 0.0\n",
        "        print (\"Evaluating\")\n",
        "\n",
        "        if load_path is not None:\n",
        "            print (f'Loading path from {load_path}')\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            batch = test[i:i+self.batch_size]\n",
        "            imgs = batch[:,0]\n",
        "            questions = list(batch[:,1])\n",
        "            labels = batch[:,2]\n",
        "            if (imgs == []) or (questions == []) or (labels == []):\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'test')\n",
        "                y_pred = torch.log_softmax(class_pred.unsqueeze(0), dim=1)\n",
        "                _, y_pred_tags = torch.max(y_pred, dim=1)\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                if (int(labels[0]) == y_pred_tags[0].item()):\n",
        "                    total_correct+=1\n",
        "                \n",
        "                predictions.append([int(labels[0]), y_pred_tags[0].item()])\n",
        "                \n",
        "        print (f\"Test accuracy is {total_correct/len(test)} \\n\")\n",
        "\n",
        "        if (return_all == True):\n",
        "            return predictions\n",
        "        else:\n",
        "            return total_correct/len(test)\n",
        "\n",
        "    def train(self, start_epoch=0, epochs=2, batch_size=1, load_path=None, save_path=None):\n",
        "\n",
        "        num_batches = int(len(train))\n",
        "        train_loss_log = []\n",
        "        test_acc_log = []\n",
        "        max_test = 0.0\n",
        "\n",
        "        print (save_path)\n",
        "\n",
        "        self.model.train()\n",
        "        \n",
        "        if load_path is not None:\n",
        "            print (load_path)\n",
        "            checkpt = torch.load(load_path)\n",
        "            train_loss_log = checkpt['loss']\n",
        "            self.model.load_state_dict(checkpt['vb_model_state_dict'])\n",
        "            start_epoch = checkpt['epoch'] + 1\n",
        "            max_test = checkpt['max_test']\n",
        "            test_acc_log = checkpt['test_acc_log']\n",
        "            batch_size = checkpt['batch_size']\n",
        "            self.lr = checkpt['lr']\n",
        "\n",
        "        for ep in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "\n",
        "            for i in range(num_batches):\n",
        "\n",
        "                assert(self.model.training)\n",
        "\n",
        "                if (i%4000==0 and i>0):\n",
        "                    print (f'Epoch {ep}, {i}/{num_batches} batches, loss is {train_loss/i}')\n",
        "        \n",
        "                batch = train[i:i+self.batch_size]\n",
        "                imgs = batch[:,0]\n",
        "                questions = list(batch[:,1])\n",
        "                labels = batch[:,2]\n",
        "\n",
        "                if (imgs == []) or (questions == []) or (labels == []):\n",
        "                    continue\n",
        "\n",
        "                class_pred = self.make_prediction(imgs, questions, 'train')\n",
        "                gt = torch.zeros(class_pred.shape).cuda()\n",
        "                gt[int(labels[0])] = 1.0\n",
        "                loss = self.loss(class_pred.unsqueeze(0), gt.unsqueeze(0))\n",
        "                loss.backward()\n",
        "                train_loss += loss.item()\n",
        "                \n",
        "                if ((i+1)%batch_size==0 or i==len(train)):\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    \n",
        "            (train_loss_log.append(train_loss))\n",
        "            \n",
        "            if (save_path != None and (ep+1)%3==0):\n",
        "                torch.save({\n",
        "                    'epoch': ep,\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': train_loss_log,\n",
        "                    'vb_model_state_dict': self.model.state_dict(),\n",
        "                    'max_test': max_test,\n",
        "                    'test_acc_log': test_acc_log,\n",
        "                    'batch_size': batch_size,\n",
        "                    'lr' : self.lr\n",
        "                }, save_path+f\"/onlytext_b{batch_size}_lr{int(self.lr)}_{ep}.pth\")\n",
        "                print (\"Saved model to: \", save_path+f\"/onlytext_b{batch_size}_lr{int(1000000*self.lr)}_{ep}.pth\")\n",
        "\n",
        "            \n",
        "            print (f'Completed {ep+1} epochs out of {epochs}, loss is {train_loss_log[ep]/len(train)} \\n')\n",
        "            test_acc = self.test(batch_size=1)\n",
        "            try:\n",
        "                if (test_acc > max_test):\n",
        "                    max_test = test_acc\n",
        "                    torch.save({\n",
        "                        'epoch': ep,\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'loss': train_loss_log,\n",
        "                        'vb_model_state_dict': self.model.state_dict(),\n",
        "                        'max_test': max_test,\n",
        "                        'test_acc_log': test_acc_log,\n",
        "                        'batch_size': batch_size,\n",
        "                        'lr': self.lr\n",
        "                    }, save_path+f\"/onlytext_b{batch_size}_lr{int(self.lr)}_best.pth\")\n",
        "                    print (\"Saved model to: \", save_path+f\"/onlytext_b{batch_size}_lr{int(1000000*self.lr)}_best.pth\")\n",
        "            except:\n",
        "                print (\"Could not check for the best model\")\n",
        "            "
      ],
      "metadata": {
        "id": "8xeH6hz9Oe6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Loading model\")\n",
        "visualbert = VisualBERTModel(len(ans2label), lr=0.00001, use_weights=False)\n",
        "\n",
        "#load visual embeddings from pickle file\n",
        "print (\"Loading visual embeddings from file\")\n",
        "visual_embeddings_path = \"/content/drive/MyDrive/CS685/project/img_features\"\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_train.pkl\", 'train')\n",
        "visualbert.load_visual_embeddings(visual_embeddings_path+\"_test.pkl\", 'test')\n",
        "\n",
        "print (\"Success\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIh0vYS3Oob2",
        "outputId": "ba28e7fa-be94-4c24-aeca-7e106ddf99ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model\n",
            "Loading visual embeddings from file\n",
            "Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_path = None #'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\n",
        "visualbert.loss = visualbert.loss.cuda()\n",
        "visualbert.train(epochs=20,batch_size=8,save_path='/content/drive/MyDrive/CS685/project/vb/onlytext',load_path=load_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AB4r3JINOsvu",
        "outputId": "3bf217b8-069b-4995-f4ca-205f857f63b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/project/vb/onlytext\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:197: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, 4000/19755 batches, loss is 5.474832760185003\n",
            "Epoch 0, 8000/19755 batches, loss is 4.527506230980507\n",
            "Epoch 0, 12000/19755 batches, loss is 4.281121228742918\n",
            "Epoch 0, 16000/19755 batches, loss is 4.107028461766705\n",
            "Completed 1 epochs out of 20, loss is 3.91989210078091 \n",
            "\n",
            "Evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:137: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 0.41436940041350795 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_best.pth\n",
            "Epoch 1, 4000/19755 batches, loss is 3.955085031014867\n",
            "Epoch 1, 8000/19755 batches, loss is 2.9828177307977604\n",
            "Epoch 1, 12000/19755 batches, loss is 2.69743658782955\n",
            "Epoch 1, 16000/19755 batches, loss is 2.6197090527741964\n",
            "Completed 2 epochs out of 20, loss is 2.4692756544705503 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.47484493452791177 \n",
            "\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_best.pth\n",
            "Epoch 2, 4000/19755 batches, loss is 3.5999522047850623\n",
            "Epoch 2, 8000/19755 batches, loss is 2.661463652789462\n",
            "Epoch 2, 12000/19755 batches, loss is 2.3852718366539047\n",
            "Epoch 2, 16000/19755 batches, loss is 2.2729843676850723\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_2.pth\n",
            "Completed 3 epochs out of 20, loss is 2.1385722459412557 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.47070985527222603 \n",
            "\n",
            "Epoch 3, 4000/19755 batches, loss is 3.3457272601533115\n",
            "Epoch 3, 8000/19755 batches, loss is 2.445868243504654\n",
            "Epoch 3, 12000/19755 batches, loss is 2.1932188605511644\n",
            "Epoch 3, 16000/19755 batches, loss is 2.078657937147129\n",
            "Completed 4 epochs out of 20, loss is 1.9435139030655824 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.44900068917987596 \n",
            "\n",
            "Epoch 4, 4000/19755 batches, loss is 3.121361016617644\n",
            "Epoch 4, 8000/19755 batches, loss is 2.2725385113260144\n",
            "Epoch 4, 12000/19755 batches, loss is 2.040368570354961\n",
            "Epoch 4, 16000/19755 batches, loss is 1.9260549976710353\n",
            "Completed 5 epochs out of 20, loss is 1.799996309297448 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.43470020675396276 \n",
            "\n",
            "Epoch 5, 4000/19755 batches, loss is 2.929758924663496\n",
            "Epoch 5, 8000/19755 batches, loss is 2.134114699700398\n",
            "Epoch 5, 12000/19755 batches, loss is 1.919532162296447\n",
            "Epoch 5, 16000/19755 batches, loss is 1.8099738205577856\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_5.pth\n",
            "Completed 6 epochs out of 20, loss is 1.6842460586323067 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4381461061337009 \n",
            "\n",
            "Epoch 6, 4000/19755 batches, loss is 2.7892582643320694\n",
            "Epoch 6, 8000/19755 batches, loss is 2.03413761762827\n",
            "Epoch 6, 12000/19755 batches, loss is 1.8288596188690553\n",
            "Epoch 6, 16000/19755 batches, loss is 1.7237360785562745\n",
            "Completed 7 epochs out of 20, loss is 1.6157218812008323 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.434355616815989 \n",
            "\n",
            "Epoch 7, 4000/19755 batches, loss is 2.652920122176056\n",
            "Epoch 7, 8000/19755 batches, loss is 1.9477990144814594\n",
            "Epoch 7, 12000/19755 batches, loss is 1.7506388380785654\n",
            "Epoch 7, 16000/19755 batches, loss is 1.6488921457835035\n",
            "Completed 8 epochs out of 20, loss is 1.5436051001771685 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.43332184700206755 \n",
            "\n",
            "Epoch 8, 4000/19755 batches, loss is 2.5260462536322077\n",
            "Epoch 8, 8000/19755 batches, loss is 1.8646093879348036\n",
            "Epoch 8, 12000/19755 batches, loss is 1.678563375618024\n",
            "Epoch 8, 16000/19755 batches, loss is 1.5806694696851393\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_8.pth\n",
            "Completed 9 epochs out of 20, loss is 1.4771920542069124 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.44693314955203306 \n",
            "\n",
            "Epoch 9, 4000/19755 batches, loss is 2.4389712859526087\n",
            "Epoch 9, 8000/19755 batches, loss is 1.799505207545096\n",
            "Epoch 9, 12000/19755 batches, loss is 1.617942624206245\n",
            "Epoch 9, 16000/19755 batches, loss is 1.5235107792823424\n",
            "Completed 10 epochs out of 20, loss is 1.4234999372873756 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.44348725017229496 \n",
            "\n",
            "Epoch 10, 4000/19755 batches, loss is 2.3670419607470623\n",
            "Epoch 10, 8000/19755 batches, loss is 1.7485815904951179\n",
            "Epoch 10, 12000/19755 batches, loss is 1.5794625068353425\n",
            "Epoch 10, 16000/19755 batches, loss is 1.4850863810937571\n",
            "Completed 11 epochs out of 20, loss is 1.3849040823779764 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.4462439696760855 \n",
            "\n",
            "Epoch 11, 4000/19755 batches, loss is 2.3031125788085447\n",
            "Epoch 11, 8000/19755 batches, loss is 1.70538115895607\n",
            "Epoch 11, 12000/19755 batches, loss is 1.5439707485265997\n",
            "Epoch 11, 16000/19755 batches, loss is 1.4506745645367651\n",
            "Saved model to:  /content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr10_11.pth\n",
            "Completed 12 epochs out of 20, loss is 1.353032647601018 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.44589937973811167 \n",
            "\n",
            "Epoch 12, 4000/19755 batches, loss is 2.255687641130268\n",
            "Epoch 12, 8000/19755 batches, loss is 1.6781220400293966\n",
            "Epoch 12, 12000/19755 batches, loss is 1.5365028974265689\n",
            "Epoch 12, 16000/19755 batches, loss is 1.4392018228846455\n",
            "Completed 13 epochs out of 20, loss is 1.3405946963424211 \n",
            "\n",
            "Evaluating\n",
            "Test accuracy is 0.45554789800137835 \n",
            "\n",
            "Epoch 13, 4000/19755 batches, loss is 2.1984345032259447\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-93ab3ca1964d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m#'/content/drive/MyDrive/CS685/project/vb/batched_best.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvisualbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/CS685/project/vb/onlytext'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-6a8c9e17650e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_epoch, epochs, batch_size, load_path, save_path)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additonal Testing/Analysis"
      ],
      "metadata": {
        "id": "wSLC9YwpXtsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path = \"/content/drive/MyDrive/PathVQA/split/\"\n",
        "# train, test, val, ans2label = load_data(path)\n",
        "label2ans = {}\n",
        "for key in ans2label:\n",
        "    label2ans[ans2label[key]] = key\n",
        "    # print (len(label2ans))"
      ],
      "metadata": {
        "id": "FQFyBQrI7YOv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.asarray(preds)\n",
        "print ((preds).shape)\n",
        "label_coverage = preds[:,0]\n",
        "print (len(set(label_coverage))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia0fOE9I-nTJ",
        "outputId": "fa6498f1-d892-4cb8-d181-16f563b08ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5804, 2)\n",
            "464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verifying test accuracies"
      ],
      "metadata": {
        "id": "C4p9mGQudF6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_paths = ['/content/drive/MyDrive/CS685/project/vb/linear_regressor/batched_best.pth',\n",
        "              '/content/drive/MyDrive/CS685/project/vb/deep_classifier/upgraded_b8_lr0_17.pth',\n",
        "              '/content/drive/MyDrive/CS685/project/vb/unfrozen/unfrozen_b8_lr0_best.pth',\n",
        "              '/content/drive/MyDrive/CS685/project/vb/weighted/unfrozen_b8_lr0_best.pth',\n",
        "              '/content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr0_best.pth',\n",
        "              '/content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr0_best.pth']"
      ],
      "metadata": {
        "id": "52CE3vNFdiD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_path = '/content/drive/MyDrive/CS685/project/vb/unfrozen/unfrozen_b8_lr0_best.pth'\n",
        "for load_path in load_paths:\n",
        "    print (load_path)\n",
        "    checkpt = torch.load(load_path)\n",
        "    try:\n",
        "        print (checkpt['lr'])\n",
        "        print (checkpt['batch_size'])\n",
        "    except:\n",
        "        print (\"Did not store lr and batch size earlier\")\n",
        "    print (checkpt['test_acc_log'])\n",
        "    print (checkpt['max_test'])\n",
        "    print (\"\\n\")\n",
        "# preds = visualbert.test(batch_size=1,load_path=load_path,return_all=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEjrpLgSdMZ1",
        "outputId": "138a927f-d425-46b0-fc30-d7396e5a1ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/project/vb/linear_regressor/batched_best.pth\n",
            "Did not store lr and batch size earlier\n",
            "[]\n",
            "0.27705031013094417\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/CS685/project/vb/deep_classifier/upgraded_b8_lr0_17.pth\n",
            "0.001\n",
            "8\n",
            "[]\n",
            "0.3282219159200551\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/CS685/project/vb/unfrozen/unfrozen_b8_lr0_best.pth\n",
            "1e-05\n",
            "8\n",
            "[]\n",
            "0.47088215024121294\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/CS685/project/vb/weighted/unfrozen_b8_lr0_best.pth\n",
            "1e-05\n",
            "8\n",
            "[]\n",
            "0.4739834596829773\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/CS685/project/vb/onlytext/onlytext_b8_lr0_best.pth\n",
            "1e-05\n",
            "8\n",
            "[]\n",
            "0.47484493452791177\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr0_best.pth\n",
            "1e-05\n",
            "8\n",
            "[]\n",
            "0.5430737422467264\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error analysis"
      ],
      "metadata": {
        "id": "hXniI0QPVZ50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "load_paths = ['/content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr0_best.pth']\n",
        "incorrect_exs = []\n",
        "\n",
        "for load_path in load_paths:\n",
        "    print(load_path)\n",
        "    gt_preds = visualbert.test(batch_size=1,load_path=load_path,return_all=True)\n",
        "    gt, preds = gt_preds[:,0], gt_preds[:,1]\n",
        "    print (f'Predicted class spans over {len(set(preds))} out of {len(set(gt))} classes in test set')\n",
        "    print (metrics.f1_score(gt, preds, average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjNj2QmsVkAS",
        "outputId": "9aff5a88-2401-4feb-ebf2-eb642293fb79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr0_best.pth\n",
            "Evaluating\n",
            "Loading path from /content/drive/MyDrive/CS685/project/vb/augmented/augmented_b8_lr0_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:141: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 0.5366988283942109 \n",
            "\n",
            "Predicted class spans over 43 out of 464 classes in test set\n",
            "0.5142479599612664\n"
          ]
        }
      ]
    }
  ]
}